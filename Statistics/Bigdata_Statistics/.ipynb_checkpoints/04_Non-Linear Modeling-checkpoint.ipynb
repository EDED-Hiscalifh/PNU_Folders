{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2ae60a",
   "metadata": {},
   "source": [
    "# 1. Non-Linear Models \n",
    "\n",
    "- Kinds of non-linear models : \n",
    "    - Polynomials \n",
    "    - Step functions \n",
    "    - Splines \n",
    "    - Local regression\n",
    "    - Generalized additive models \n",
    "- Population : $y_i = \\sum_{j=1}^p f_j(x_j) + \\epsilon_i$ \n",
    "- Inference : $\\hat{y}_i = \\sum_{j=1}^p \\hat{f}_j(x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6c6f7",
   "metadata": {},
   "source": [
    "# 2. Polynomial Regression \n",
    "\n",
    "- Not really intersted in the coefficients.  \n",
    "- More interested in fitted function values. \n",
    "- $\\hat{f}(x_0) = \\hat{\\beta_0} + \\hat{\\beta_1}x_0 + ... + \\hat{\\beta_4}x_0^4$\n",
    "- Compute the fit and pointwise standard error(confidence interval) : $\\hat{f}(x_0) +- 2se[\\hat{f}(x_0)]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610533e6",
   "metadata": {},
   "source": [
    "## 2.1 [Ex] Polynomial Regression of wage dataset \n",
    "\n",
    "```R\n",
    "# install.packages('ISLR')\n",
    "\n",
    "# Import library \n",
    "library(ISLR)\n",
    "attach(Wage)\n",
    "\n",
    "# Orthogonal polynomials:\n",
    "# Each column is a linear orthogonal combination of\n",
    "# age, age^2, age^3 and age^4\n",
    "fit <- lm(wage ~ poly(age, 4), data=Wage)\n",
    "summary(fit)\n",
    "# Direct power of age\n",
    "fit2 <- lm(wage ~ poly(age, 4, raw=T), data=Wage)\n",
    "summary(fit2)\n",
    "fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), Wage)\n",
    "fit2b <- lm(wage ~ cbind(age, age ^2, age^3, age^4), Wage)\n",
    "round(data.frame(fit=coef(fit), fit2=coef(fit2),\n",
    "                 fit2a=coef(fit2a), fit2b=coef(fit2b)), 5)\n",
    "\n",
    "# Testing datasets \n",
    "age.grid <- seq(min(age), max(age))\n",
    "# se=TRUE, extract sd(f(x)) from fhat  \n",
    "preds <- predict(fit, newdata=list(age=age.grid), se=TRUE)\n",
    "\n",
    "# Calcualte confidence band\n",
    "se.bands <- cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)\n",
    "\n",
    "# Visualize results\n",
    "plot(age, wage, xlim=range(age), cex=.5, col=\"darkgrey\")\n",
    "title(\"Degree-4 Polynomial\", outer=T)\n",
    "lines(age.grid, preds$fit, lwd=2, col=\"darkblue\")\n",
    "matlines(age.grid, se.bands, lwd=2, col=\"darkblue\", lty=2)\n",
    "```\n",
    "\n",
    "![](Img/Poly1.png)\n",
    "\n",
    "- The more narrower width of confidence bands, the more stable values of prediction we can get. \n",
    "- Checking the confidence interval graph, it can be seen that the area of the interval was narrow for the data that existsed in the training dataset, but the prediction accuracy was poor for the data that was completely new. \n",
    "\n",
    "```R\n",
    "# Orthogonal vs. Non-orthogonal polynomial regression\n",
    "preds2 <- predict(fit2, newdata=list(age=age.grid), se=TRUE)\n",
    "data.frame(fit=preds$fit, fit2=preds2$fit)\n",
    "sum(abs(preds$fit-preds2$fit))\n",
    "```\n",
    "\n",
    "- The difference of prediction between orthogonal vs non-orthogonal polynomial regression is just 1.103956e-09."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf906c",
   "metadata": {},
   "source": [
    "## 2.2 [Ex] Choose optimal polynomial terms : Statistics \n",
    "\n",
    "```R\n",
    "# Anova test to find the optimal polynomial degree\n",
    "fit.1 <- lm(wage ~ age, data=Wage)\n",
    "fit.2 <- lm(wage ~ poly(age, 2), data=Wage)\n",
    "fit.3 <- lm(wage ~ poly(age, 3), data=Wage)\n",
    "fit.4 <- lm(wage ~ poly(age, 4), data=Wage)\n",
    "fit.5 <- lm(wage ~ poly(age, 5), data=Wage)\n",
    "g <- anova(fit.1, fit.2, fit.3, fit.4, fit.5)\n",
    "g\n",
    "```\n",
    "<img src=\"Img/Poly2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "- The ANOVA test will perform comparison of two models. \n",
    "- In case of Model 2, \n",
    "    - $H_0 : \\beta_2 = 0$ : Model 1 is more preferable. \n",
    "    - $H_1 : \\beta_2 \\neq 0$ : Model 2 is more preferable.\n",
    "- We can choose adequate polynomial terms from this test. \n",
    "\n",
    "```R\n",
    "# Perform T-test\n",
    "coef(summary(fit.5))\n",
    "round(coef(summary(fit.5)), 5)\n",
    "\n",
    "# T-test^2 = F-test\n",
    "summary(fit.5)$coef[-c(1, 2), 3]\n",
    "summary(fit.5)$coef[-c(1, 2), 3]^2\n",
    "g$F[-1]\n",
    "```\n",
    "\n",
    "![](Img/Poly3.png)\n",
    "\n",
    "- With T-test with fit.5 models, we can choose optimal polynomial terms. \n",
    "- The preferable polynomial terms will be 3($\\beta_3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6f25a",
   "metadata": {},
   "source": [
    "## 2.3 [Ex] Choose optimal polynomial terms : K-fold CV simulation\n",
    "\n",
    "```R\n",
    "# 10-fold cross-validation to choose the optimal polynomial\n",
    "set.seed(1111)\n",
    "N <- 10 # simulation replications\n",
    "K <- 10 # 10-fold CV\n",
    "\n",
    "# Simulation Error matrix \n",
    "CVE <- matrix(0, N, 10)\n",
    "\n",
    "# Model training - Calculate test error \n",
    "for (k in 1:N) {\n",
    "    gr <- sample(rep(seq(K), length=nrow(Wage)))\n",
    "    # Cross Validation Error matrix \n",
    "    pred <- matrix(NA, nrow(Wage), 10)\n",
    "    for (i in 1:K) {\n",
    "        tran <- (gr != i)\n",
    "        test <- (gr == i)\n",
    "        for (j in 1:10) {\n",
    "            g <- lm(wage ~ poly(age, j), data=Wage, subset=tran)\n",
    "            yhat <- predict(g, data.frame(poly(age, j)))\n",
    "            mse <- (Wage$wage - yhat)^2\n",
    "            pred[test, j] <- mse[test]\n",
    "        }\n",
    "    }\n",
    "    CVE[k, ] <- apply(pred, 2, mean)\n",
    "}\n",
    "RES <- apply(CVE, 2, mean)\n",
    "RES\n",
    "\n",
    "# Visualize Result\n",
    "par(mfrow=c(1,2))\n",
    "matplot(t(CVE), type=\"l\",xlab=\"Degrees of Polynomials \",\n",
    "        ylab=\"Mean Squared Error\")\n",
    "plot(seq(10), RES, type=\"b\", col=2, pch=20, \n",
    "     xlab=\"Degrees of Polynomials \", ylab=\"Mean Squared Error\")\n",
    "```\n",
    "\n",
    "![](Img/NonLinear1.png)\n",
    "\n",
    "- When we check the graph, we can see that the CVE value significantly decreases when the polynomial term increase from 1 to 2.\n",
    "- When building a model, it is important to use the lease expensive one with statistically significant results, rather than choosing the one with the lowest CVE value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862436d6",
   "metadata": {},
   "source": [
    "## 2.4 [Ex] Logistic Regression\n",
    "\n",
    "```R\n",
    "# Logistic regression using binary response\n",
    "# 1 for wage > 250 and 0 for wage <= 250\n",
    "fit <- glm(I(wage>250) ~ poly(age, 4), Wage, family=\"binomial\")\n",
    "\n",
    "# Predict dataset and calculate confidence bands \n",
    "preds <- predict(fit, newdata=list(age=age.grid), se=T)\n",
    "# logit transformation \n",
    "pfit <- exp(preds$fit) / (1 + exp(preds$fit))\n",
    "# calculate confidence bands \n",
    "se.bands.logit <- cbind(preds$fit + 2*preds$se.fit, preds$fit - 2*preds$se.fit)\n",
    "se.bands <- exp(se.bands.logit)/(1 + exp(se.bands.logit))\n",
    "\n",
    "# Another function \n",
    "preds2 <- predict(fit, newdata=list(age=age.grid), type=\"response\", se=T)\n",
    "cbind(pfit, preds2$fit)\n",
    "\n",
    "# Visualize result of logistic regression \n",
    "plot(age , I(wage > 250), xlim=range(age), type=\"n\", ylim=c(0, .2))\n",
    "points(jitter(age), I((wage>250)/5), cex=.5, pch=\"|\", col=\"darkgrey\")\n",
    "lines(age.grid, pfit, lwd=2, col=\"darkblue\")\n",
    "matlines(age.grid, se.bands, lwd=2, col=\"darkblue\", lty=2)\n",
    "```\n",
    "\n",
    "![](Img/NonLinear2.png)\n",
    "\n",
    "When the wage target was changed to TRUE/FALSE classification problem based on 250 and the model was fit, it can be seen that the confidence interval was narrow in the case of values existing in the training dataset, but the width of the confidence interval increased significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa70c4",
   "metadata": {},
   "source": [
    "# 3. Step Functions\n",
    "\n",
    "- Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. Cubic regression uses three variables $X_1$, $X_2$, $X_3$ as predictors. This is a simple way to provide a non-linear fit to the data. \n",
    "- Step functions cut the range of a variable into K distinct regions in order to produce a qualitative variable. This has the effect of fitting a piece wise constant function. \n",
    "\n",
    "Using polynomial functions of the features as predictors imposes a global structure on the non-linear function of X. Instead, we could use step functions to avoid such global structure. Here we break X into bins, and fit a different constant in each bin. Essentially, we create the bins by selecting K cut-points in the range of X, and then construct K+1 new variables, which behave like dummy variables.    \n",
    "   \n",
    " \n",
    "**Source from :** https://rstudio-pubs-static.s3.amazonaws.com/24589_7552e489485b4c2790ea6634e1afd68d.html\n",
    "\n",
    "- Cut the variable into distinct regions : $C_1(X) = I(X < c_1), C_2(X) = I(c_1 \\leq X < c_2), ..., C_K(X) = I(X \\geq c_K)$ \n",
    "- For any value of $X$, $C_0(X) + C_1(X) + ... C_k(X) = 1$ \n",
    "- Choice of cutpoints or **knots** can be problematic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa8f5f",
   "metadata": {},
   "source": [
    "## 3.1 [Ex] Step Functions with linearity\n",
    "\n",
    "```R\n",
    "# cut() automatically picked the cut points \n",
    "table(cut(age, 4)) \n",
    "# (17.9, 33.5] (33.5, 49] (49, 64.5] (64.5, 80.1]\n",
    "\n",
    "# Linear regression\n",
    "fit <- lm(wage ~ cut(age, 4), data=Wage) \n",
    "# Logistic regression \n",
    "fit2 <- glm(I(wage > 250) ~ cut(age, 4), data=Wage, family=\"binomial\") \n",
    "\n",
    "# The age < 33.5 category is left out. \n",
    "# The first category is recognized as reference. \n",
    "coef(summary(fit)) \n",
    "```\n",
    "\n",
    "![](Img/NonLinear4.png)\n",
    "\n",
    "```R\n",
    "# Fitted values along with confidence bands. \n",
    "# confidence bands of linear regression \n",
    "age.grid <- seq(min(age), max(age))\n",
    "preds <- predict(fit, newdata=list(age=age.grid), se=TRUE)\n",
    "se.bands <- cbind(preds$fit + 2*preds$se.fit, preds$fit - 2*preds$se.fit)\n",
    "\n",
    "# confidence bands of logistic regression \n",
    "preds2 <- predict(fit2, newdata=list(age=age.grid), se=T)\n",
    "pfit <- exp(preds2$fit)/(1 + exp(preds2$fit))\n",
    "se.bands.logit <- cbind(preds2$fit + 2*preds2$se.fit, preds2$fit - 2*preds2$se.fit)\n",
    "se.bands2 <- exp(se.bands.logit)/(1 + exp(se.bands.logit))\n",
    "\n",
    "# Visualize result \n",
    "# Linear regression \n",
    "par(mfrow=c(1,2), mar=c(4.5 ,4.5 ,1 ,1), oma=c(0, 0, 4, 0))\n",
    "plot(age, wage, xlim=range(age), cex=.5, col=\"darkgrey\")\n",
    "title(\"Degree-4 Step Functions\", outer=T)\n",
    "lines(age.grid, preds$fit, lwd=3, col=\"darkgreen\")\n",
    "matlines(age.grid, se.bands, lwd=2, col=\"darkgreen\", lty=2)\n",
    "\n",
    "# Logistic regression \n",
    "plot(age , I(wage > 250), xlim=range(age), type=\"n\", ylim=c(0, .2))\n",
    "points(jitter(age), I((wage >250)/5), cex=.5, pch=\"|\", col=\"darkgrey\")\n",
    "lines(age.grid, pfit, lwd=3, col=\"darkgreen\")\n",
    "matlines(age.grid, se.bands2, lwd=2, col=\"darkgreen\", lty=2)\n",
    "```\n",
    "![](Img/NonLinear3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87186597",
   "metadata": {},
   "source": [
    "## 3.2 Piecewise Polynomials \n",
    "\n",
    "- Instead of a single polynomials in $X$ over, we can use different polynomials in regions defined by knots. \n",
    "- The value of the continuous variable of $X$ is separated by to apply the polynomial regression for each continuous value. \n",
    "- $y_i$\n",
    "    - $y_i = \\beta_{01} + \\beta_{11}x_i + \\beta_{21}x_i^2 + \\beta_{31}x_i^3 + \\epsilon_i$, if $x_i < c$\n",
    "    - $y_i = \\beta_{02} + \\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_i^3 + \\epsilon_i$, if $x_i \\geq c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f72a17",
   "metadata": {},
   "source": [
    "## 3.3 [Ex] Step Functions with non-linearity\n",
    "\n",
    "\n",
    "```R\n",
    "# 200 obs. are randomly generated from 3000 obs \n",
    "set.seed(19) \n",
    "ss <- sample(3000, 200)\n",
    "nWage <- Wage[ss,]\n",
    "\n",
    "# testing sets \n",
    "age.grid <- seq(min(nWage$age), max(nWage$age)) \n",
    "\n",
    "# Training model in piecewise polynomials \n",
    "g1 <- lm(wage ~ poly(age, 3), data=nWage, subset=(age < 50))\n",
    "g2 <- lm(wage ~ poly(age, 3), data=nWage, subset=(age > 50)) \n",
    "\n",
    "# Predict on testing dataset\n",
    "pred1 <- predict(g1, newdata=list(age=age.grid[age.grid < 50]))\n",
    "pred2 <- predict(g2, newdata=list(age=age.grid[age.grid >= 50]))\n",
    "         \n",
    "# Visualize result\n",
    "par(mfrow = c(1, 2))\n",
    "plot(nWage[, 2], nWage[, 11], col=\"darkgrey\", xlab=\"Age\",\n",
    "ylab=\"Wage\")\n",
    "title(main = \"Piecewise Cubic\")\n",
    "lines(age.grid[age.grid < 50], pred1, lwd=2, col=\"darkblue\")\n",
    "lines(age.grid[age.grid >= 50], pred2, lwd=2, col=\"darkblue\")\n",
    "abline(v=50, lty=2)\n",
    "```\n",
    "\n",
    "- Using piecewise polynomials, discontinuities occur for each point(knots) separated from the continuous value.\n",
    "- To solve this problem, continuous piecewise polynomials are calculated by additionally calculating LHS and RHS(the effect of removing one coefficients)\n",
    "\n",
    "```R\n",
    "# Define the two hockey-stick functions\n",
    "LHS <- function(x) ifelse(x < 50, 50-x, 0)\n",
    "RHS <- function(x) ifelse(x < 50, 0, x-50)\n",
    "\n",
    "# Fit continuous piecewise polynomials\n",
    "g3 <- lm(wage ~ poly(LHS(age), 3) + poly(RHS(age), 3), nWage)\n",
    "pred3 <- predict(g3, newdata=list(age=age.grid))\n",
    "plot(nWage[, 2], nWage[, 11], col=\"darkgrey\", xlab=\"Age\",\n",
    "     ylab=\"Wage\")\n",
    "title(main=\"Continuous Piecewise Cubic\")\n",
    "lines(age.grid, pred3, lwd=2, col=\"darkgreen\")\n",
    "abline(v=50, lty=2)\n",
    "\n",
    "summary(g1)\n",
    "summary(g2)\n",
    "summary(g3)\n",
    "```\n",
    "\n",
    "![](Img/NonLinear5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d9916",
   "metadata": {},
   "source": [
    "# 4. Splines \n",
    "\n",
    "Regression splines are more flexible than polynomials and step functions, and are actually an extension of the two. The divide the range of X into K distinct regions. For each region, a polynomial function is fit to the data, however, the polynomials are constrained so that they join smoothly at the region boundaries or knots. \n",
    "\n",
    "For regression splines, instead of fitting a high degree polynomial over the entire of X, we can fit several low-degree polynomials over different regions of X. Each of these functions can be fit using lease squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254efbcf",
   "metadata": {},
   "source": [
    "## 4.1 Linear Splines \n",
    "\n",
    "- A Linear splines with knots at $\\zeta_k$, $k = 1, ..., K$ is a piecewise linear polynomial continuous at each knot. \n",
    "- $y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_{K+1}b_{K+1}(x_i) + \\epsilon$ \n",
    "    - $b_1(x_i) = x_i$\n",
    "    - $b_{k+1}(x_i) = (x_i - \\zeta_k)_{+}$\n",
    "    - $(x_i - \\zeta_k)_{+} = x_i - \\zeta_k$, if $x_i > \\zeta_k$\n",
    "    \n",
    "    \n",
    "```R\n",
    "# Linear spline\n",
    "g6 <- lm(wage ~ bs(age, knots=50, degree=1), data=nWage)\n",
    "pred6 <- predict(g6, newdata=list(age=age.grid))\n",
    "\n",
    "plot(nWage[, 2], nWage[, 11], col=\"darkgrey\", xlab=\"Age\",\n",
    "     ylab=\"Wage\")\n",
    "title(main=\"Linear Spline\")\n",
    "lines(age.grid, pred6, lwd=2, col=\"darkred\")\n",
    "abline(v=50, lty=2)\n",
    "```\n",
    "\n",
    "![](Img/NonLinear7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0089e",
   "metadata": {},
   "source": [
    "## 4.2 Cubic Splines \n",
    "\n",
    "- A Cubic spline with knots at $\\zeta_k$, $k = 1, ..., K$ is a piecewise cubic polynomial with contunuous derivatives up to order 2 at each knot. \n",
    "- $y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_2(x_i) + ... + \\beta_{K+3}b_{K+3}(x_i) + \\epsilon$ \n",
    "    - $b_1(x_i) = x_i$\n",
    "    - $b_2(x_i) = x_i^2$\n",
    "    - $b_3(x_i) = x_i^3$\n",
    "    - $b_{k+3}(x_i) = (x_i - \\zeta_k)_{+}^3$\n",
    "    - $(x_i - \\zeta_{k+3})_{+}^3 = (x_i - \\zeta_k)^3$, if $x_i > \\zeta_k$\n",
    "    \n",
    "```R\n",
    "# Truncated power basis functions \n",
    "d <- 3\n",
    "knots <- 50 \n",
    "x1 <- outer(nWage$age, 1:3, \"^\") # make form of x, x^2, x^3 \n",
    "x2 <- outer(nWage$age, knots, \">\") * outer(nwage$age, knots, \"-\")^d # make form of (x-z)^3 \n",
    "x <- cbind(x1, x2) # make form of x, x^2, x^3, (x-50)^3\n",
    "\n",
    "# Train models \n",
    "g4 <- lm(wage ~ x, data=nWage)\n",
    "\n",
    "# Make testing set and predictions \n",
    "nx1 <- outer(age.grid, 1:d, \"^\")\n",
    "nx2 <- outer(age.grid, knots, \">\") * outer(age.grid, knots, \"-\")^d\n",
    "nx <- cbind(nx1, nx2)\n",
    "pred4 <- predict(g4, newdata=list(x=nx))\n",
    "\n",
    "# Visualize \n",
    "par(mfrow=c(1,2))\n",
    "plot(nWage[, 2], nWage[, 11], col=\"darkgrey\", xlab=\"Age\", ylab=\"Wage\")\n",
    "title(main = \"Cubic Spline\")\n",
    "lines(age.grid, pred4, lwd=2, col=\"red\")\n",
    "abline(v=50, lty = 2)\n",
    "\n",
    "# Make automatic splines using bs function \n",
    "library(splines)\n",
    "g5 <- lm(wage ~ bs(age, knots=50), data=nWage)\n",
    "pred5 <- predict(g5, newdata=list(age=age.grid))\n",
    "plot(nWage[, 2], nWage[, 11], col=\"darkgrey\", xlab=\"Age\", ylab=\"Wage\")\n",
    "title(main=\"Cubic Spline\")\n",
    "lines(age.grid, pred5, lwd=2, col=\"red\")\n",
    "abline(v=50, lty=2)\n",
    "```\n",
    "![](Img/NonLinear6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b318182",
   "metadata": {},
   "source": [
    "## 4.3 Natural Cubic Splines \n",
    "\n",
    "- Splines have high variance at the outer range of the predictors. \n",
    "- A natural splines is a regression spline with additional boundary constraints. \n",
    "- The natural function is required to be linear at the boundary. \n",
    "\n",
    "```R\n",
    "# Cubic Spline\n",
    "fit <- lm(wage ~ bs(age, knots=c(25 ,40 ,60)), data=nWage)\n",
    "pred <- predict(fit, newdata=list(age=age.grid), se=T)\n",
    "\n",
    "# Natural Spline : using(ns) function \n",
    "fit2 <- lm(wage ~ ns(age, knots=c(25 ,40 ,60)), data=nWage)\n",
    "pred2 <- predict(fit2, newdata=list(age=age.grid), se=T)\n",
    "plot(nWage[, 2], nWage[, 11], col=\"darkgrey\", xlab=\"Age\", ylab=\"Wage\")\n",
    "lines(age.grid, pred$fit, lwd=2, col=4)\n",
    "lines(age.grid, pred$fit + 2*pred$se, lty=\"dashed\", col=4)\n",
    "lines(age.grid, pred$fit - 2*pred$se, lty=\"dashed\", col=4)\n",
    "lines(age.grid, pred2$fit, lwd=2, col=2)\n",
    "lines(age.grid, pred2$fit + 2*pred2$se, lty=\"dashed\", col=2)\n",
    "lines(age.grid, pred2$fit - 2*pred2$se, lty=\"dashed\", col=2)\n",
    "abline(v=c(25, 40, 60), lty=2)\n",
    "legend(\"topright\", c(\"Natural Cubic Spline\", \"Cubic Spline\"), lty=1, lwd=2, col=c(2, 4))\n",
    "```\n",
    "\n",
    "![](Img/NonLinear8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d73675",
   "metadata": {},
   "source": [
    "### [Ex] Natural Cubic Splines with degree of freedom \n",
    "\n",
    "- We can train data with natural cubic splines based on percentile using ns(df=k) keywords. \n",
    "- We fit a natrual cubic spline with three knots, where the knots locations were chosen automatically as the 25th, 50th, and 75th percentiles. \n",
    "\n",
    "```R\n",
    "# Use a complete Wage data \n",
    "age <- Wage$age \n",
    "wage <- Wage$wage \n",
    "\n",
    "# Make test dataset \n",
    "age.grid <- seq(min(age), max(age)) \n",
    "\n",
    "# Training model with natural cubic splines \n",
    "g1 <- lm(wage ~ ns(age, df=4), data=Wage) \n",
    "pred1 <- predict(g1, newdata=list(age=age.grid), se=T) \n",
    "se.bands1 <- cbind(pred1$fit + 2*pred1$se.fit, pred1$fit - 2*pred1$se.fit) \n",
    "\n",
    "# Training model with natural cubic splines of binary problems \n",
    "g2 <- glm(I(wage > 25) ~ ns(age, df=4), data=Wage, family=\"binomial\") \n",
    "pred2 <- predict(g2, newdata=list(age=age.grid), se=T) \n",
    "pfit <- exp(pred2$fit)/(1 + exp(pred2$fit)) \n",
    "se.bands.logit <- cbind(pred2$fit + 2*pred2$se.fit, pred2$fit - 2*pred2$se.fit)\n",
    "se.bands2 <- exp(se.bands.logit)/(1 + exp(se.bands.logit)) \n",
    "\n",
    "# Visualize predicted confidence bands : lm fit \n",
    "par(mfrow=c(1,2), mar=c(4.5 ,4.5 ,1 ,1), oma=c(0, 0, 4, 0))\n",
    "plot(age, wage, cex=.5, col=\"darkgrey\", xlab=\"Age\", ylab=\"Wage\")\n",
    "title(\"Natural Cubic Spline\", outer=T)\n",
    "lines(age.grid, pred1$fit, lwd=3, col=2)\n",
    "matlines(age.grid, se.bands1, lwd=2, col=2, lty=2)\n",
    "ncs <- ns(age, df=4)\n",
    "attr(ncs, \"knots\")\n",
    "abline(v=attr(ncs, \"knots\"), lty=3)\n",
    "\n",
    "# Visualize predicted confidence bands : glm fit \n",
    "plot(age, I(wage > 250), type =\"n\", ylim=c(0, .2), xlab=\"Age\",\n",
    "     ylab=\"Pr(Wage>250 | Age)\")\n",
    "points(jitter(age), I((wage >250)/5), cex=.5, pch =\"|\",\n",
    "       col=\"darkgrey\")\n",
    "lines(age.grid, pfit, lwd=3, col=2)\n",
    "matlines(age.grid, se.bands2, lwd=2, col=2, lty=2)\n",
    "abline(v=attr(ncs, \"knots\"), lty=3)\n",
    "```\n",
    "\n",
    "- Cubic splines with $K$ knots has $K+4$ parameters or df. \n",
    "\n",
    "<img src=\"Img/NonLinear9.png\" width=400 height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb40e6",
   "metadata": {},
   "source": [
    "### [Ex] Optimized Natural Cubic Splines \n",
    "\n",
    "- Place more knots where the function might vary most rapidly. \n",
    "- Place fewer knots where it seems more stable. \n",
    "- How many knots should we use : Using a cross-validation\n",
    "\n",
    "```R\n",
    "set.seed(1111)\n",
    "CVE <- matrix(0, 20, 10) \n",
    "\n",
    "# Iterate 20 times of Cross Validation \n",
    "for (k in 1:20) { \n",
    "    gr <- sample(rep(seq(10), length=nrow(Wage))) \n",
    "    pred <- matrix(NA, nrow(Wage), 10) \n",
    "    # Applying 10 K-folds Cross Validation \n",
    "    for (i in 1:10) {\n",
    "        tran <- (gr != i) \n",
    "        test <- (gr == i) \n",
    "        # Natural Cubic Splines with j degree of freedom \n",
    "        for (j in 1:10) {\n",
    "            nsx <- ns(age, df=j) \n",
    "            g <- lm(wage ~ nsx, data=Wage, subset=tran) \n",
    "            mse <- (Wage$wage - predict(g, nsx))^2\n",
    "            pred[test, j] <- mse[test]\n",
    "        } \n",
    "    }\n",
    "    CVE[k, ] <- apply(pred, 2, mean)\n",
    "}\n",
    "RES <- apply(CVE, 2, mean)\n",
    "plot(seq(10), RES, type=\"b\", col=2, pch=20, xlab=\"Degrees of Freedom of Natural Spline\", \n",
    "     ylab=\"Mean Squared Error\")   \n",
    "```\n",
    "\n",
    "![](Img/NonLinear10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3baa3",
   "metadata": {},
   "source": [
    "### [Ex] Comparison to Polynomial Regression \n",
    "\n",
    "- Regression splines often give superior results to polynomial regression. \n",
    "- The extra flexibility in the polynomial produces undesirable results at the boundaries, while the natural cubic spline still provides a reasonable fit to the data. \n",
    "\n",
    "```R\n",
    "# Training model with natural cubic splines with 15 df and polynomail with 15 terms \n",
    "g1 <- lm(wage ~ ns(age, df=15), data=Wage)\n",
    "g2 <- lm(wage ~ poly(age, 15), data=Wage)\n",
    "\n",
    "# Make prediction of testing set\n",
    "pred1 <- predict(g1, newdata=list(age=age.grid), se=T)\n",
    "pred2 <- predict(g2, newdata=list(age=age.grid), se=T)\n",
    "\n",
    "# Visualize comparison results \n",
    "plot(age, wage, cex=.5, col=\"darkgrey\", xlab=\"Age\", ylab=\"Wage\")\n",
    "lines(age.grid, pred1$fit, lwd=2, col=2)\n",
    "lines(age.grid, pred1$fit + 2*pred1$se, lty=\"dashed\", col=2)\n",
    "lines(age.grid, pred1$fit - 2*pred1$se, lty=\"dashed\", col=2)\n",
    "lines(age.grid, pred2$fit, lwd=2, col=4)\n",
    "lines(age.grid, pred2$fit + 2*pred2$se, lty=\"dashed\", col=4)\n",
    "lines(age.grid, pred2$fit - 2*pred2$se, lty=\"dashed\", col=4)\n",
    "legend(\"topleft\", c(\"Natural Cubic Spline\", \"Polynomial\"), lty=1, lwd=2, col=c(2, 4))\n",
    "```\n",
    "\n",
    "![](Img/NonLinear11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da82eb",
   "metadata": {},
   "source": [
    "## 4.4 Smoothing Splines \n",
    "\n",
    "- Finding a function $g(x)$ that make $RSS$ small, but that is also smooth. \n",
    "- A function $g(x)$ that minimizes below is a smoothing spline. \n",
    "    - $\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int g(t)^2dt $ \n",
    "    - $\\lambda$ is a non-negative tuning paramete. \n",
    "    - $\\lambda$ controls how wiggly $g(x)$ is.    \n",
    "    - The smaller $\\lambda$, the more wiggly the function. \n",
    "    - As $\\lambda \\to \\infty$, the function $g(x)$ becomes linear. \n",
    "- The solution is a natural cubic spline, with a knot at every unique value of $x_i$. \n",
    "    - Smoothing splines avoid the knot-selection issue, leaving a single $\\lambda$ to be choosen. \n",
    "    - The function smooth.spline() will fit a smoothing spline. \n",
    "    - The leave-one-out cross-validation error(LOOCV) can be computed very efficiently for smoothing splines \n",
    "    - $LOOCV_{\\lambda} = \\sum_{i=1}^n(y_i - \\hat{g}_{\\lambda}^{[-i]}(x_i))^2 = \\sum_{i=1}^n[\\frac{y_i - \\hat{g}_{\\lambda}(x_i)}{1 - S_{\\lambda ii}}]^2$\n",
    "    \n",
    "    \n",
    "```R\n",
    "# Preparing dataset\n",
    "library(ILSR)\n",
    "library(splines)\n",
    "data(Wage)\n",
    "age <- Wage$age\n",
    "wage <- Wage$wage\n",
    "age.grid <- seq(min(age), max(age)) \n",
    "\n",
    "# Training models fitted in smoothing splines \n",
    "# Polynomial fits with df = 16\n",
    "fit <- smooth.spline(age, wage, df=16) \n",
    "\n",
    "# Optimized smoothing splines with cross validation\n",
    "fit2 <- smooth.spline(age, wage, cv=TRUE) \n",
    "fit2$df \n",
    "\n",
    "# Natural cubic splines with df=7\n",
    "fit3 <- lm(wage ~ ns(age, df=7), data=Wage)\n",
    "pred3 <- predict(fit3, newdata=list(age=age.grid)) \n",
    "\n",
    "# Visualize fits \n",
    "plot(age, wage, cex=.5, col=\"darkgrey\") \n",
    "title(\"Smoothing Spline vs Natural Spline\") \n",
    "lines(fit, col=\"red\", lwd=2)\n",
    "lines(fit2, col=\"blue\", lwd=2) \n",
    "lines(age.grid, pred3, col=\"green\", lwd=2) \n",
    "legend(\"topright\", legend=c(\"SS (DF=16)\", \"SS (DF=6.8)\", \"NS (DF=7)\"), col=c(\"red\",\"blue\",\"green\"), lty=1, lwd=2)\n",
    "\n",
    "# Cross Validation Simulation\n",
    "set.seed(1234)\n",
    "N <- 10 # Simulation replications\n",
    "K <- 10 # 10-fold CV\n",
    "df <- seq(2, 20) ## Degrees of freedom\n",
    "CVE <- matrix(0, N, length(df))\n",
    "for (k in 1:N) {\n",
    "    gr <- sample(rep(seq(K), length=nrow(Wage)))\n",
    "    pred <- matrix(NA, nrow(Wage), length(df))\n",
    "    for (i in 1:K) {\n",
    "        tran <- (gr != i)\n",
    "        test <- (gr == i)\n",
    "        for (j in 1:length(df)) {\n",
    "            fit <- smooth.spline(age[tran], wage[tran], df=df[j])\n",
    "            mse <- (wage-predict(fit, age)$y)^2\n",
    "            pred[test, j] <- mse[test]\n",
    "        }\n",
    "    }\n",
    "CVE[k, ] <- apply(pred, 2, mean)\n",
    "}\n",
    "RES <- apply(CVE, 2, mean)\n",
    "\n",
    "# Visualize Cross Validation Error \n",
    "par(mfrow=c(1,2))\n",
    "matplot(t(CVE), type=\"b\", col=2, lty=2, pch=20, ylab=\"CV errors\", xlab=\"Degrees of freedom\")\n",
    "plot(df, RES, type=\"b\", col=2, pch=20, ylab=\"averaged CV errors\", xlab=\"Degrees of freedom\")\n",
    "abline(v=df[which.min(RES)], col=\"grey\", lty=4)\n",
    "```\n",
    "\n",
    "![](Img/NonLinear12.png)\n",
    "\n",
    "```R\n",
    "# Smoothng splines vs Natural Cubic Splines \n",
    "set.seed(1357)\n",
    "MSE1 <- matrix(0, 100, 2)\n",
    "for (i in 1:100) {\n",
    "    tran <- sample(nrow(Wage), size=floor(nrow(Wage)*2/3))\n",
    "    test <- setdiff(1:nrow(Wage), tran)\n",
    "    g1 <- smooth.spline(age[tran], wage[tran], df=7)\n",
    "    g2 <- lm(wage ~ ns(age, df=7), data=Wage, subset=tran)\n",
    "    mse1 <- (wage-predict(g1, age)$y)[test]^2\n",
    "    mse2 <- (wage-predict(g2, Wage))[test]^2\n",
    "    MSE1[i,] <- c(mean(mse1), mean(mse2))\n",
    "}\n",
    "apply(MSE1, 2, mean)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7eeea",
   "metadata": {},
   "source": [
    "# 5. Local Regression\n",
    "\n",
    "- **Local regression** or **local polynomial regression**, also known as moving regression, is a generalization of the moving average and polynomial regression. \n",
    "- Local regression computes the fit at target upoint $x_0$ using only the regression nearby training obervation. \n",
    "- With a sliding weight function, we fit separate linear fits over the range of $x$ by weighted least squares. \n",
    "- OLS : $\\hat{\\beta}^{OLS} = (X^{'}X)^{-1}X^{'}y$\n",
    "- WLS : $\\hat{\\beta}^{WLS} = (X^{'}WX)^{-1}X^{'}W^{-1}y$ \n",
    "- GLS : $\\hat{\\beta}^{GLS} = (X^{'}\\Omega^{-1} X)^{-1}X^{'}\\Omega^{-1}y$ \n",
    "\n",
    "![](Img/NonLinear13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92d0be",
   "metadata": {},
   "source": [
    "## 5.1 [Ex] Local Regeression \n",
    "\n",
    "```R\n",
    "# Prepare datasets\n",
    "data(Wage)\n",
    "age <- Wage$age\n",
    "wage <- Wage$wage\n",
    "age.grid <- seq(min(age), max(age)) \n",
    "\n",
    "# Training model \n",
    "fit1 <- loess(wage ~ age, span=.2, data=wage)\n",
    "fit2 <- loess(wage ~ age, span=.3, data=wage) \n",
    "\n",
    "# Visualize fitted local regression model \n",
    "plot(age, wage, cex =.5, col = \"darkgrey\")\n",
    "title(\"Local Linear Regression\")\n",
    "lines(age.grid, predict(fit1, data.frame(age=age.grid)),\n",
    "col=\"red\", lwd=2)\n",
    "lines(age.grid, predict(fit2, data.frame(age=age.grid)),\n",
    "col=\"blue\", lwd=2)\n",
    "legend(\"topright\", legend = c(\"Span = 0.2\", \"Span = 0.7\"),\n",
    "col=c(\"red\", \"blue\"), lty=1, lwd=2)\n",
    "```\n",
    "\n",
    "![](Img/NonLinear14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d71084",
   "metadata": {},
   "source": [
    "## 5.2 [Ex] Coparison of Polynomial, Natural Cubic Spline, Local Regression\n",
    "\n",
    "```R\n",
    "set.seed(1357)\n",
    "MSE2 <- matrix(0, 100, 2)\n",
    "for (i in 1:100) {\n",
    "    # Train-Test split \n",
    "    tran <- sample(nrow(Wage), size=floor(nrow(Wage)*2/3))\n",
    "    test <- setdiff(1:nrow(Wage), tran)\n",
    "    # Training model based on value of span \n",
    "    g1 <- loess(wage ~ age, span=.2, data=Wage)\n",
    "    g2 <- loess(wage ~ age, span=.7, data=Wage)\n",
    "    # Calculate MSE result of testing set \n",
    "    mse1 <- (wage-predict(g1, Wage))[test]^2\n",
    "    mse2 <- (wage-predict(g2, Wage))[test]^2\n",
    "    MSE2[i,] <- c(mean(mse1), mean(mse2))\n",
    "}\n",
    "MSE <- cbind(MSE1, MSE2)\n",
    "apply(MSE, 2, mean)\n",
    "apply(MSE, 2, sd)\n",
    "\n",
    "# Visualize results \n",
    "boxplot(MSE, boxwex=0.5, col=4:7, ylim=c(1200, 2000), \n",
    "        names= c(\"Smoothing Spline\", \"Natural Cubic\", \"Local (S=0.2)\", \"Local (S=0.7)\"), \n",
    "        ylab=\"Mean Squared Errors\")\n",
    "```\n",
    "\n",
    "![](Img/NonLinear15.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5fdee",
   "metadata": {},
   "source": [
    "# 6. Generalized Additive Models\n",
    "\n",
    "- **Generalized additive models(GAMs)** allows non-linear functions of each of the variables, while maintaining additivity.\n",
    "- $y_i = \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + ... + \\epsilon_i$\n",
    "- It is called an additive model because we calcualte a separate $f_j$ fore each $x_j$, and then add together all of their contributions. \n",
    "- **GAMs** provide a useful-compromise between linear and fully nonparametric models.\n",
    "\n",
    "## 6.1 [Ex] GAMs using gam package \n",
    "\n",
    "```R\n",
    "# Prepare library and dataset \n",
    "library(ISLR)\n",
    "library(splines) \n",
    "data(Wage)\n",
    "\n",
    "# Training model \n",
    "gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data=Wage)\n",
    "summary(gam1)\n",
    "library(gam)\n",
    "\n",
    "# s() : smoothing spline\n",
    "gam <- gam(wage ~ s(year, 4)+s(age, 5)+education, data=Wage)\n",
    "\n",
    "# Visualize fitted model \n",
    "par(mfrow =c(1,3))\n",
    "plot(gam, se=TRUE, col=\"blue\", scale=70)\n",
    "plot.Gam(gam1, se = TRUE, col = \"red\")\n",
    "```\n",
    "\n",
    "![](Img/NonLinear16.png)\n",
    "\n",
    "```R\n",
    "# Significant test \n",
    "gam.m1 <- gam(wage ~ s(age, 5) + education, data=Wage)\n",
    "gam.m2 <- gam(wage ~ year + s(age, 5) + education, data=Wage)\n",
    "anova(gam.m1, gam.m2, gam, test = \"F\")\n",
    "summary(gam)\n",
    "```\n",
    "\n",
    "- The way of selecting features is using ANOVA F-test. \n",
    "- Using Group Lasso is recommended. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc74b69b",
   "metadata": {},
   "source": [
    "## 6.2 Simulation Study \n",
    "\n",
    "```R\n",
    "set.seed(1357)\n",
    "MSE3 <- matrix(0, 100, 3)\n",
    "for (i in 1:100) {\n",
    "    tran <- sample(nrow(Wage), size=floor(nrow(Wage)*2/3))\n",
    "    test <- setdiff(1:nrow(Wage), tran)\n",
    "    g1 <- gam(wage ~ s(age, 5) + education, data=Wage, subset=tran)\n",
    "    g2 <- gam(wage ~ year + s(age, 5) + education, data=Wage, subset=tran)\n",
    "    g3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data=Wage, subset=tran)\n",
    "    mse1 <- (wage - predict(g1, Wage))[test]^2\n",
    "    mse2 <- (wage - predict(g2, Wage))[test]^2\n",
    "    mse3 <- (wage - predict(g3, Wage))[test]^2\n",
    "    MSE3[i,] <- c(mean(mse1), mean(mse2), mean(mse3))\n",
    "}\n",
    "apply(MSE3, 2, mean)\n",
    "```\n",
    "\n",
    "- The result of MSE3 : 1265.564, 1261.940, 1262.823\n",
    "- The performance of gam(wage ~ s(age, 5) + education) works worst. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
