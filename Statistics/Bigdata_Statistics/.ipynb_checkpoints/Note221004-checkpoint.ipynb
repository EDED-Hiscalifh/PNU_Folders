{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Selecting the Tuning Parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 [Ex] Validation Set \n",
    "\n",
    "- alpha = 0 : ridge regression \n",
    "- alpha = 1 : lasso regression \n",
    "\n",
    "```R\n",
    "# Make dataset \n",
    "library(glmnet)\n",
    "library(ISLR) \n",
    "names(Hitters) \n",
    "Hitters <- na.omit(Hitters) \n",
    "\n",
    "set.seed(123)\n",
    "x <- model.matrix(Salary~., Hitters)[, -1] \n",
    "y <- Hitters$Salary\n",
    "\n",
    "# Train-Test Split\n",
    "train <- sample(1:nrow(x), nrow(x)/3) \n",
    "test <- (-train) \n",
    "y.test <- y[test]\n",
    "\n",
    "# Hyperparameter tuning \n",
    "grid <- 10^seq(10, -2, length=100) \n",
    "r1 <- glmnet(x[train, ], y[train], alpha=0, lambda=grid)\n",
    "ss <- 0:(length(r1$lambda)-1) \n",
    "Err <- NULL\n",
    "\n",
    "# Cross validation Error for test sample \n",
    "for (i in 1:length(r1$lambda)) { \n",
    "    r1.pred <- predict(r1, s=ss[i], newx=x[test, ])\n",
    "    Err[i] <- mean((r1.pred - y.test)^2) \n",
    "} \n",
    "wh <- which.min(Err) \n",
    "lam.opt <- r1$lambda[wh] \n",
    "\n",
    "# Get full model with optimized hyperparmeter \n",
    "r.full <- glmnet(x, y, alpha=0, lambda=grid) \n",
    "r.full$beta[, wh] \n",
    "predict(r.full, type=\"coefficients\", s=lam.opt) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 [Ex] K-fold Cross Validation \n",
    "\n",
    "```R\n",
    "set.seed(1234)\n",
    "cv.r <- cv.glmnet(x, y, alpha=0, nfolds=10)\n",
    "names(cv.r) \n",
    "# cvm : The mean value of cross validation -> CVE \n",
    "# cvsd : The standard deviation of cross validation -> One-standard error \n",
    "# cvup : The upperbound of CVE -> cvm + cvsd \n",
    "# cvlo : The lowerbound of CVE -> cvm - cvsd \n",
    "# lambda.min : The lambda which optimize input model \n",
    "# lambda.1se : The lambda which optimize imput model based on one-standard error \n",
    "\n",
    "cbind(cv.r$cvlo, cv.r$cvm, cv.r$cvup)\n",
    "# Scatter plot based on One-Standard error \n",
    "# left vertix line : log(lambda.min) \n",
    "# right vertix line(more shrinked model) : log(lambda.1se) \n",
    "plot(cv.r) \n",
    "\n",
    "which(cv.r$lambda==cv.r$lambda.min)\n",
    "which(cv.r$lambda==cv.r$lambda.1se)\n",
    "# 100, 54 -> lambda.min < lambda.1se \n",
    "\n",
    "b.min <- predict(cv.r, type=\"coefficients\", s=cv.r$lambda.min)\n",
    "b.1se <- predict(cv.r, type=\"coefficients\", s=cv.r$lambda.1se)\n",
    "\n",
    "# calculate l1-norm\n",
    "# calculate sum(b.min!=0) - 1 to get l2-norm \n",
    "cbind(b.min, b.1se)\n",
    "c(sum(b.min[-1]^2), sum(b.1se[-1]^2))\n",
    "# sum(b.min[-1]^2) > sum(b.1se[-1]^2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Consider reality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 The Bias-Variance tradeoff\n",
    "\n",
    "- If lambda axis increases to the right \n",
    "- Overfittng vs Underfitting \n",
    "- (Low bias + High variance) vs (High bias + Low variance)\n",
    "- (l1-norm, l2-norm increase) vs (l1-norm, l2-norm decrease) \n",
    "- $\\lambda$ decrease vs $\\lambda$ increase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Comparison between Lasso and Ridge \n",
    "\n",
    "- If nonzero coefficient are large, ridge is better. \n",
    "- If nonzero coefficient are small, lasso is better. \n",
    "- In high-dimensional data where spares model is assummed, lasso perform better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Regularization Methods \n",
    "\n",
    "- Regularization methods are based on a penalized likelihood : \n",
    "    - $Q_{\\lambda}(\\beta_0, \\beta) = -l(\\beta_0, \\beta) + p_{\\lambda}(\\beta)$\n",
    "    - $(\\hat{\\beta_0}, \\hat{\\beta}) = arg min Q_{\\lambda}(\\beta_0, \\beta)$ for a fixed $\\lambda$.\n",
    "- Penalized likelihood for quantitive \n",
    "    - Linear regression model : $y_i = \\beta_0 + x_i^T \\beta + \\epsilon_i$ \n",
    "    - l1-norm : $\\lambda \\sum(\\hat{\\beta}^2)$ \n",
    "    - l2-norm : $\\lambda \\sum|\\hat{\\beta}|$ \n",
    "    - $Q_{\\lambda}(\\beta_0, \\beta) = -l(\\beta_0, \\beta) + p_{\\lambda}(\\beta) = \\frac{1}{2}\\sum_{i=1}^{n}(y_i - \\beta_0 + x_i^T \\beta)^2 +  p_{\\lambda}(\\beta)$\n",
    "- Penalized likelyhood for binary \n",
    "    - \n",
    "    - CVE based on deviance : \n",
    "    - CVE based on classification error : $CVE = \\frac{1}{n}\\sum\\sum I(y_i - \\hat{y_i})^{[-k]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 [Ex] Heart Data(Binary Classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
