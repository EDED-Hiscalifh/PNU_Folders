{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddce555",
   "metadata": {},
   "source": [
    "# 1. Tree-based Methods \n",
    "\n",
    "- **Tree-based methods** for regression and classification involve stratifying or segmenting the predictor space into a number of simple regions. \n",
    "- The set of splitting rules used to segment the predictor space can be summarized in a tree. \n",
    "- Tree-based methods are simple and useful for interpretation. \n",
    "- **Bagging** and **Random Forests** grow multiple trees which are combined to yield a single consensus prediction.\n",
    "- Combining a large number of trees can result in dramatic improvements in prediction accuracy, at the expense of some loss interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9540d3e4",
   "metadata": {},
   "source": [
    "# 2. Decision Tree \n",
    "\n",
    "- At a given internal node, the label (of the form ($X_j < t_k$) indicates the left-hand branch emanting from that split, and the right-hand branch corresponds to $X_j \\geq t_k$. \n",
    "- The number in each leaf is the mean of the response for the observation that fall there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58826f0e",
   "metadata": {},
   "source": [
    "## 2.1 [Ex] Decision Trees using different packages \n",
    "\n",
    "```R\n",
    "# Import library and dataset \n",
    "library(ISLR)\n",
    "library(tree)\n",
    "data(Hitters)\n",
    "str(Hitters)\n",
    "\n",
    "# Missing data\n",
    "summary(Hitters$Salary)\n",
    "miss <- is.na(Hitters$Salary)\n",
    "\n",
    "# Fit a regression tree\n",
    "g <- tree(log(Salary) ~ Years + Hits, subset=!miss, Hitters)\n",
    "g\n",
    "summary(g)\n",
    "\n",
    "# Draw a tree\n",
    "plot(g)\n",
    "text(g, pretty=0)\n",
    "```\n",
    "\n",
    "<img src=\"Img/Tree1.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "**tree(formula, data, weights, subsets, x, y)** \n",
    "\n",
    "A tree is grown by binary recursive partitioning using the response in the specified formula and choosing splits from the terms of the right-hand-side. \n",
    "\n",
    "\n",
    "```R\n",
    "# Prune a tree\n",
    "g2 <- prune.tree(g, best=3)\n",
    "plot(g2)\n",
    "text(g2, pretty=0)\n",
    "```\n",
    "<img src=\"Img/Tree2.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "**prune.tree(tree, k=NULL, best=NULL, newdata)**\n",
    "\n",
    "Determines a nested sequence of subtrees of the supplied tree by recursively \"snipping\" off the least important splits. \n",
    "\n",
    "- tree : fitted model object of class 'tree'. \n",
    "- best : integer requesting the size of a specific subtree in the cost-complexity sequence to be returned. \n",
    "\n",
    "```R\n",
    "# Another R package for tree\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "# Fit a regression tree\n",
    "g3 <- rpart(log(Salary) ~ Years + Hits, subset=!miss, Hitters)\n",
    "g3\n",
    "summary(g3)\n",
    "\n",
    "# Draw a fancy tree\n",
    "prp(g3, branch.lwd=4, branch.col=\"darkgreen\", extra=101)\n",
    "```\n",
    "\n",
    "<img src=\"Img/Tree3.png\" width=\"400\" height=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39577c",
   "metadata": {},
   "source": [
    "# 3. Details of the Tree-building Process \n",
    "\n",
    "- Decision trees are typically drawn upside down, in the sense that the **leaves** are at the bottom of the tree. \n",
    "- The points along the tree where the predictor space is split are referred to as **internal nodes**. \n",
    "- The predictor space : \n",
    "    - Set of possible values : $X_1, ..., X_p$ \n",
    "    - Non-overlapping regions : $R_1, ..., R_j$ \n",
    "- For every observation that falls into the region $R_j$, make the same prediction which is simply the mean of the response values for the training observations in $R_j$. \n",
    "- Choose to divide the predictor space into high-dimensional rectangles. \n",
    "\n",
    "**Goal** : Find boxes $R_1, ..., R_j$ that minimize the RSS($\\sum_{j=1}^J\\sum_{i\\in R_j}(y_i - \\hat{y}_{R_j})$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c638ef1",
   "metadata": {},
   "source": [
    "## 3.1 Recursive Binary Splitting \n",
    "\n",
    "- Creating a binary decision tree is actually a process of dividing up the input space. \n",
    "- A greedy approach is used to divide the space called recursive binary splitting. \n",
    "- It is **greedy** because at each step of the tree-building process, the best split is made at that particular step rather than picking a split that will lead to a better tree in future step. \n",
    "\n",
    "\n",
    "1. Select the predictor $X_j$ such that splitting the predictor space into the regions {$X|X_j < s$} and {$X|X_j \\geq s$} leads to the greatest possible reduction in RSS.\n",
    "2. Repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. \n",
    "3. The process continues until a stopping criterion is reached : no region contains more than five observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22119dab",
   "metadata": {},
   "source": [
    "## 3.2 Pruning a Tree \n",
    "\n",
    "- The process with many splits may produce good predictions on the training set, but is likely to **overfit** the data, leading to poor testing set performance. \n",
    "- A smaller tree with fewer splits might lead **lower variance** and better interpretation at the cost of a little bias. \n",
    "- Grow a very large tree $T_0$, and then prune it back in order to obtain a subtree. \n",
    "- **Cost complexity pruning(weakest link pruning)** : $\\sum_{m=1}^{|T|}\\sum_{i:x_i \\in R_m}(y_i - \\hat{y}R_m)^2 + \\alpha|T|$ \n",
    "- $\\alpha$ controls a trade-off between the subtree's complexity and its fit to the training data. \n",
    "- Select an optimal value $\\hat{\\alpha}$ using Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c3ac2",
   "metadata": {},
   "source": [
    "## 3.4 Summary : Tree Algorithm\n",
    "\n",
    "1. Use **recursive binary splitting** to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations(**stopping criterion**).\n",
    "2. Apply **cost complexity pruning** to the large tree in order to obtain a sequence of best subtrees, as a function of $\\alpha$.\n",
    "    - We can use prune.tree(tree, best=K) function \n",
    "3. Use K-fold cross-validation to choose $\\alpha$. For each $k = 1, . . . , K$:\n",
    "    - Repeat Steps 1 and 2 on the $\\frac{K - 1}{K}$ th fraction of the training data, excluding the $k$th fold.\n",
    "    - Evaluate the mean squared prediction error on the data in the left-out $k$th fold, as a function of $\\alpha$.\n",
    "    - Average the results, and pick $\\alpha$ to minimize the average error.\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93b39d",
   "metadata": {},
   "source": [
    "# 4. Regression Decision Tree : Hitters Data, Boston Data \n",
    "\n",
    "## 4.1 [Ex] Find optimal value $\\hat{\\alpha}$ using CV\n",
    "```R\n",
    "# Import library and dataset\n",
    "library(tree)\n",
    "data(Hitters) \n",
    "\n",
    "# Training models \n",
    "miss <- is.na(Hitters$Salary) \n",
    "g <- tree(log(Salary) ~ Years + Hits + RBI + PutOuts + Walks + Runs + Assists + HmRun + Errors + Atbat, subset=!miss, Hitters) \n",
    "\n",
    "# Perform 6 fold CV\n",
    "set.seed(1234)\n",
    "cv.g <- cv.tree(g, K=6) \n",
    "plot(cv.g$size, cv.g$dev, type=\"b\")\n",
    "```\n",
    "\n",
    "![](Img/Tree4.png)\n",
    "\n",
    "```R\n",
    "# Find the optmial tree size that minimze MSE\n",
    "w <- which.min(cv.g$dev)\n",
    "g2 <- prune.tree(g, best=cv.g$size[w]) \n",
    "plog(g2)\n",
    "text(g2, pretty=0)\n",
    "```\n",
    "\n",
    "![](Img/Tree5.png)\n",
    "\n",
    "## 4.2 Workflow of optimizing unpruned tree\n",
    "\n",
    "1. Make tree model using tree function : g <- tree(format, subset=train, data)\n",
    "2. Perform K-fold CV using cv.tree function : cv.g <- cv.tree(tree, K=k) \n",
    "3. Find optimized tree size : w <- which.min(cv.g\\$dev) \n",
    "4. Prune tree using prune.tree function : g2 <- prune.tree(g, best=cv.g$size[w]) \n",
    "5. Make prediction using predict function : yhat <- predict(g2, testset)\n",
    "6. Calculate MSE : sqrt(mean((yhat-tree.test)^2))\n",
    "\n",
    "This is optimized model using metrics as 6-fold CV. The optimized alpha is stored in cv.g$size[w]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dae5de",
   "metadata": {},
   "source": [
    "## 4.3 [Ex] Calculating $MSE_{test}$ with different models \n",
    "\n",
    "**Only using tree function** \n",
    "\n",
    "```R\n",
    "# Training sets and Test sets solitting \n",
    "attach(Hitters)\n",
    "newdata <- data.frame(Salary, Years, Hits, RBI, PutOuts, Walks, Runs, Assists, HmRun, Errors, AtBat)\n",
    "newdata <- newdata[!miss, ]\n",
    "\n",
    "# Separate samples into 132 training sets and 131 test sets \n",
    "set.seed(1111) \n",
    "train <- sample(1:nrow(newdata), ceiling(nrow(newdata)/2)) \n",
    "\n",
    "# Fit a tree with training set and compute test MSE\n",
    "tree.train <- tree(log(Salary) ~ ., subset=train, newdata)\n",
    "yhat1 <- exp(predict(tree.train, newdata[-train, ]))\n",
    "tree.test <- newdata[-train, \"Salary\"]\n",
    "\n",
    "# Visualize results\n",
    "plot(yhat1, tree.test)\n",
    "abline(0,1)\n",
    "sqrt(mean((yhat1-tree.test)^2))\n",
    "```\n",
    "\n",
    "![](Img/Tree6.png)\n",
    "\n",
    "- xaxis : predicted values of regression decision tree\n",
    "- yaxis : real values of newdata[test]\n",
    "- MSE : 376.4349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc16c28",
   "metadata": {},
   "source": [
    "**Using tree with optimized alpha(best)**\n",
    "\n",
    "```R\n",
    "# Perform 6 fold CV for training sets\n",
    "set.seed(1234)\n",
    "cv.g <- cv.tree(tree.train, K=6)\n",
    "plot(cv.g$size, cv.g$dev, type=\"b\")\n",
    "\n",
    "# Prune a tree with training set and compute test MSE\n",
    "# in the original sclae\n",
    "w <- which.min(cv.g$dev)\n",
    "prune.tree <- prune.tree(tree.train, best=cv.g$size[w])\n",
    "yhat2 <- exp(predict(prune.tree, newdata[-train, ]))\n",
    "plot(yhat2, tree.test)\n",
    "abline(0,1)\n",
    "sqrt(mean((yhat2-tree.test)^2))\n",
    "```\n",
    "\n",
    "![](Img/Tree7.png)\n",
    "\n",
    "- MSE : 367.4892"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55fdc7",
   "metadata": {},
   "source": [
    "**Using linear regression(lm function)**\n",
    "\n",
    "```R\n",
    "# Compute test MSE of least square estimates\n",
    "g0 <- lm(log(Salary)~., newdata, subset=train)\n",
    "yhat3 <- exp(predict(g0, newdata[-train,]))\n",
    "sqrt(mean((yhat3-newdata$Salary[-train])^2))\n",
    "```\n",
    "\n",
    "- MSE : 407.3643"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159e3b9",
   "metadata": {},
   "source": [
    "## 4.4 [Ex] Building Regression Decision Tree \n",
    "\n",
    "**Prequirisite** \n",
    "\n",
    "```R\n",
    "library(MASS)\n",
    "data(Boston)\n",
    "str(Boston) \n",
    "```\n",
    "\n",
    "**Training model with tree function on training set** \n",
    "\n",
    "```R\n",
    "set.seed(1)\n",
    "train <- sample(1:nrow(Boston), nrow(Boston)/2)\n",
    "tree.boston <- tree(medv ~ ., Boston, subset=train)\n",
    "summary(tree.boston)\n",
    "plot(tree.boston)\n",
    "text(tree.boston, pretty=0)\n",
    "```\n",
    "\n",
    "![](Img/Tree8.png)\n",
    "\n",
    "**Training model with optimized alpha(best) on training set** \n",
    "\n",
    "```R\n",
    "cv.boston <- cv.tree(tree.boston, K=5)\n",
    "plot(cv.boston$size, cv.boston$dev, type=\"b\")\n",
    "which.min(cv.boston$dev)\n",
    "```\n",
    "\n",
    "![](Img/Tree9.png)\n",
    "\n",
    "**Calculate MSE of tree model** \n",
    "\n",
    "```R\n",
    "yhat <- predict(tree.boston, newdata=Boston[-train, ])\n",
    "boston.test <- Boston[-train, \"medv\"]\n",
    "plot(yhat, boston.test)\n",
    "abline(0, 1)\n",
    "mean((yhat - boston.test)^2)\n",
    "```\n",
    "\n",
    "- MSE : 35.28688\n",
    "\n",
    "**Calculate MSE of linear regression model** \n",
    "\n",
    "```R\n",
    "g <- lm(medv ~ ., Boston, subset=train)\n",
    "pred <- predict(g, Boston[-train,])\n",
    "mean((pred - boston.test)^2)\n",
    "```\n",
    "\n",
    "- MSE : 26.86123\n",
    "\n",
    "**Calculate MSE of regsubsets model** \n",
    "\n",
    "```R\n",
    "library(leaps)\n",
    "g1 <- regsubsets(medv ~ ., data=Boston, nvmax=13, subset=train)\n",
    "ss <- summary(g1)\n",
    "cr <- cbind(ss$adjr2, ss$cp, ss$bic)\n",
    "x.test <- as.matrix(Boston[-train, -14])\n",
    "MSE <- NULL\n",
    "for (i in 1:3) {\n",
    "    beta <- rep(0, ncol(Boston))\n",
    "    if (i > 1) ww <- which.min(cr[,i])\n",
    "    else ww <- which.max(cr[,i])\n",
    "    beta[ss$which[ww,]] <- coef(g1, ww)\n",
    "    preds <- cbind(1, x.test) %*% beta\n",
    "    MSE[i] <- mean((preds - boston.test)^2)\n",
    "}\n",
    "MSE\n",
    "```\n",
    "\n",
    "- MSE of Adjusted R^2 : 26.85842\n",
    "- MSE of AIC : 26.85842\n",
    "- MSE of BIC : 29.10294"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29949dc3",
   "metadata": {},
   "source": [
    "## 4.5 [Ex] Assignments \n",
    "\n",
    "- The exhaustive variable selection method found 13 best models such that : summary(g1)$which[, -1] \n",
    "- For each best model, fit a regression tree with the same training set and compute MSE of the same test set.\n",
    "- Find the best model that minimize the test MSE among 13 models. Do not prune your regression tree. \n",
    "\n",
    "```R\n",
    "# Importing Library\n",
    "library(MASS)\n",
    "data(Boston)\n",
    "library(leaps)\n",
    "library(tree)\n",
    "\n",
    "# Train-Test Splitting \n",
    "set.seed(1)\n",
    "train <- sample(1:nrow(Boston), nrow(Boston)/2)\n",
    "\n",
    "# Training model using regsubsets \n",
    "g1 <- regsubsets(medv ~ ., data=Boston, nvmax=13, subset=train)\n",
    "\n",
    "MSE <- NULL\n",
    "for (i in 1:13) { \n",
    "  x <- which(summary(g1)$which[i, -1] == 1)\n",
    "  newdata <- data.frame(Boston[, c(x, 14)])\n",
    "  tree.train <- tree(medv ~ ., newdata, subset=train)\n",
    "  yhat <- predict(tree.train, newdata[-train, ])\n",
    "  tree.test <- newdata[-train, \"medv\"]\n",
    "  MSE[i] <- mean((yhat - tree.test)^2)\n",
    "}\n",
    "MSE\n",
    "\n",
    "wm <- which.min(MSE)\n",
    "wm\n",
    "```\n",
    "\n",
    "- wm : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76ef1b",
   "metadata": {},
   "source": [
    "# 5. Classification Tree\n",
    "\n",
    "- Predict a **qualitative response** rather than a quantitative one. \n",
    "- Predict that each observation belongs to the most commonly occuring class. \n",
    "- Use recursive binary splitting to grow a classification tree. \n",
    "- Use **classification error rate(missclassification rate)** as evaluation metrics. \n",
    "\n",
    "**Splitting metrics** \n",
    "\n",
    "- The classification error rate : $Error = 1 - max_{k}(\\hat{p}_{mk})$\n",
    "- **Gini index** \n",
    "    - Term : $G_m = \\sum_{k=1}^K\\hat{p}_{mk}(1 - \\hat{p}_{mk})$\n",
    "    - The gini inxex is referred to as a measure of node purity. \n",
    "- **Cross-entropy**\n",
    "    - Term : $C_m = -\\sum_{k=1}^K \\hat{p}_{mk} log(\\hat{p}_{mk})$\n",
    "    - $\\hat{p}_{mk} = \\frac{n_{mk}}{n_m}$\n",
    "- **Deviance**\n",
    "    - Term : $C_m = -\\sum_{k=1}^K n_{mk} log(\\hat{p}_{mk})$\n",
    "    - $n_{mk}$ : The number of observations in the $m$th node that belong to the $k$th class. \n",
    "    - Residual mean deviance : $\\frac{2}{n - |T_0|}\\sum_m D_m$\n",
    "    - $|T_o|$ : The size of the tree "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aee8e6",
   "metadata": {},
   "source": [
    "## 5.1 [Ex] Classification Decision Tree of Heart Data \n",
    "\n",
    "**Prerequirisite** \n",
    "\n",
    "```R\n",
    "# Importing dataset \n",
    "url.ht <- \"https://www.statlearning.com/s/Heart.csv\"\n",
    "Heart <- read.csv(url.ht, h=T)\n",
    "\n",
    "# Preview dataset \n",
    "summary(Heart)\n",
    "\n",
    "# Preprocessing dataset \n",
    "Heart <- Heart[, colnames(Heart)!=\"X\"]\n",
    "Heart[,\"Sex\"] <- factor(Heart[,\"Sex\"], 0:1, c(\"female\", \"male\"))\n",
    "Heart[,\"Fbs\"] <- factor(Heart[,\"Fbs\"], 0:1, c(\"false\", \"true\"))\n",
    "Heart[,\"ExAng\"] <- factor(Heart[,\"ExAng\"], 0:1, c(\"no\", \"yes\"))\n",
    "Heart[,\"ChestPain\"] <- as.factor(Heart[,\"ChestPain\"])\n",
    "Heart[,\"Thal\"] <- as.factor(Heart[,\"Thal\"])\n",
    "Heart[,\"AHD\"] <- as.factor(Heart[,\"AHD\"])\n",
    "\n",
    "# Basic EDA \n",
    "summary(Heart)\n",
    "dim(Heart)\n",
    "sum(is.na(Heart))\n",
    "Heart <- na.omit(Heart)\n",
    "dim(Heart)\n",
    "summary(Heart)\n",
    "```\n",
    "\n",
    "**Visualize proportion of heart disease rate by classes** \n",
    "\n",
    "```R\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "g1 <-ggplot(Heart, aes(x=Sex, fill=AHD)) + geom_bar(position=\"stack\")\n",
    "g2 <-ggplot(Heart, aes(x=ChestPain,fill=AHD)) + geom_bar(position=\"stack\")\n",
    "g3 <-ggplot(Heart, aes(x=Fbs, fill=AHD)) + geom_bar(position=\"stack\")\n",
    "g4 <-ggplot(Heart, aes(x=ExAng, fill=AHD)) + geom_bar(position=\"stack\")\n",
    "g5 <-ggplot(Heart,aes(x=Thal, fill=AHD)) + geom_bar(position=\"stack\")\n",
    "grid.arrange(g1, g2, g3, g4, g5, nrow=2)\n",
    "```\n",
    "\n",
    "![](Img/Tree10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0ccac",
   "metadata": {},
   "source": [
    "**Training using logistic regression model**\n",
    "\n",
    "```R\n",
    "g <- glm(AHD ~., family=\"binomial\", Heart)\n",
    "summary(g)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ccb84",
   "metadata": {},
   "source": [
    "**Training using classification decision tree model** \n",
    "\n",
    "```R\n",
    "library(tree)\n",
    "tree.heart <- tree(AHD ~., Heart)\n",
    "summary(tree.heart)\n",
    "tree.heart\n",
    "plot(tree.heart)\n",
    "text(tree.heart)\n",
    "plot(tree.heart)\n",
    "text(tree.heart, pretty=0)\n",
    "```\n",
    "\n",
    "![](Img/Tree11.png) \n",
    "\n",
    "- Values of terminal nodes(leaf nodes) are fixed of Yes/No. \n",
    "- If the conditional result of node is True move toward left subtree nodes \n",
    "- The reason why splitting occurs under the subtrees of $Ca < 0.5$ is that we use splitting metrics as gini index and cross-entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cfdc27",
   "metadata": {},
   "source": [
    "**Calculating missclassification rate of training set** \n",
    "\n",
    "```R\n",
    "# predict the probability of each class or class type\n",
    "predict(tree.heart, Heart)\n",
    "predict(tree.heart, Heart, type=\"class\")\n",
    "\n",
    "# Compute classification error rate of training observations\n",
    "pred <- predict(tree.heart, Heart, type=\"class\")\n",
    "table(pred, Heart$AHD)\n",
    "mean(pred!=Heart$AHD)\n",
    "```\n",
    "\n",
    "|pred|No|Yes|\n",
    "|:---:|:---:|:---:|\n",
    "|No|152|21|\n",
    "|Yes|8|116|\n",
    "\n",
    "- Missclassification rate : 0.0976431"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ffd84",
   "metadata": {},
   "source": [
    "## 5.2 [Ex] Calculating Missclassification of Validation Set \n",
    "\n",
    "```R\n",
    "# Separate training and test sets\n",
    "set.seed(123)\n",
    "train <- sample(1:nrow(Heart), nrow(Heart)/2)\n",
    "test <- setdiff(1:nrow(Heart), train)\n",
    "heart.test <- Heart[test, ]\n",
    "\n",
    "# Training model \n",
    "heart.tran <- tree(AHD ~., Heart, subset=train)\n",
    "\n",
    "# Make prediction of validation set \n",
    "heart.pred <- predict(heart.tran, heart.test, type=\"class\")\n",
    "\n",
    "# Compute classification error rate\n",
    "table(heart.pred, Heart$AHD[test])\n",
    "mean(heart.pred!=Heart$AHD[test])\n",
    "```\n",
    "\n",
    "|heart.pred| No | Yes | \n",
    "|:---:|:---:|:---:|\n",
    "|No| 59 | 22 | \n",
    "|Yes| 19 | 49 | \n",
    "\n",
    "- Missclassification rate : 0.2751678"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e41e3",
   "metadata": {},
   "source": [
    "## 5.3 [Ex] Pruning using Cross Validation \n",
    "\n",
    "```R\n",
    "# Run 5-fold cross validation \n",
    "set.seed(1234)\n",
    "cv.heart <- cv.tree(heart.tran, FUN=prune.missclass, K=5) \n",
    "cv.heart \n",
    "\n",
    "# Visualize cross validation missclassification rate \n",
    "par(mfrow=c(1,2)) \n",
    "plot(cv.heart$size, cv.heart$dev, type=\"b\") \n",
    "plot(cv.herat$k, cv.heart$dev, type=\"b\") \n",
    "```\n",
    "\n",
    "![](Img/Tree12.png)\n",
    "\n",
    "```R\n",
    "# Find the optimal tree size \n",
    "w <- cv.heart$size[which.min(cv.heart$dev)] \n",
    "\n",
    "# Prune the tree with the optimal size \n",
    "prune.heart <- prune.missclass(heart.tran, best=w) \n",
    "\n",
    "# Visualize unpruned tree vs pruned tree \n",
    "par(mfrow=c(1,2)) \n",
    "plot(heart.tran) \n",
    "text(heart.tran) \n",
    "plot(prune.heart) \n",
    "text(prune.heart, pretty=0)  \n",
    "```\n",
    "\n",
    "<img src=\"Img/Tree13.png\" width=\"600\" height=\"250\">\n",
    "\n",
    "```R\n",
    "# Compute classification error of the subtree \n",
    "heart.pred <- predict(prune.heart, heart.test, type=\"class\") \n",
    "table(heart.pred, Heart$AHD[test]) \n",
    "mean(heart.pred!=Heart$AHD[test]) \n",
    "```\n",
    "\n",
    "- Missclassification rate of unpruned tree : 0.2751678\n",
    "- Missclassification rate of pruned tree : 0.261745 \n",
    "- The missclassification rate aren't differ between unpruned tree and pruned tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5274a1",
   "metadata": {},
   "source": [
    "## 5.4 [Ex] Simulation Study : Iterating 100 times \n",
    "\n",
    "```R\n",
    "set.seed(111) \n",
    "K <- 100 \n",
    "RES1 <- matrix(0, K, 2) \n",
    "\n",
    "# Iterate 100 times \n",
    "for (i in 1:K) {\n",
    "    train <- sample(1:nrow(Heart), floor(nrow(Heart)*2/3)) \n",
    "    test <- setdiff(1:nrow(Heart), train) \n",
    "    heart.test <- Heart[test, ]\n",
    "    heart.tran <- tree(AHD ~., Heart, subset=train) \n",
    "    heart.pred <- predict(heart.tran, heart.test, type=\"class\") \n",
    "    RES1[i, 1] <- mean(heart.pred != Heart$AHD[test]) \n",
    "    cv.heart <- cv.tree(heart.tran, FUN=prune.misclass, K=5) \n",
    "    w <- cv.heart$size[which.min(cv.heart$dev)] \n",
    "    prune.heart <- prune.misclass(heart.tran, best=w) \n",
    "    heart.pred.cv <- predict(prune.heart, heart.test, type=\"class\") \n",
    "    RES1[i, 2] <- mean(heart.pred.cv != Heart$AHD[test]) \n",
    "} \n",
    "           \n",
    "# Calculate mean CVE of 100 iterations \n",
    "apply(RES1, 2, mean) \n",
    "boxplot(RES1, col=c(\"orange\", \"lightblue\"), boxwex=0.6, names=c(\"unpruned tree\", \"pruned tree\"),\n",
    "        ylab=\"Classification Error Rate\")                  \n",
    "```\n",
    "\n",
    "- Result of unpruned vs pruned tree : 0.2404040 vs 0.2351515 \n",
    "\n",
    "<img src=\"Img/Tree14.png\" width=\"800\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8efdce5",
   "metadata": {},
   "source": [
    "## 5.5 [Ex] Simulation Study : Iterating 100 times and comparing with other models \n",
    "\n",
    "```R\n",
    "library(MASS) \n",
    "library(e1071) \n",
    "\n",
    "set.seed(111) \n",
    "K <- 100\n",
    "RES2 <- matrix(0, K, 4) \n",
    "\n",
    "for (i in 1:K) { \n",
    "    train <- sample(1:nrow(Heart), floor(nrow(Heart)*2/3)) \n",
    "    test <- setdiff(1:nrow(Heart), train) \n",
    "    y.test <- Heart$AHD[test] \n",
    "    \n",
    "    # Logisitc Regression \n",
    "    g1 <- glm(AHD~., data=Heart, family=\"binomial\", subset=train) \n",
    "    p1 <- predict(g1, Heart[test,], type=\"response\") \n",
    "    pred1 <- rep(\"No\", length(y.test)) \n",
    "    pred1[p1 > 0.5] <- \"Yes\" \n",
    "    RES2[i, 1] <- mean(pred1!=y.test) \n",
    "    \n",
    "    # LDA/QDA \n",
    "    g2 <- lda(AHD~., data=Heart, subset=train) \n",
    "    g3 <- qda(AHD~., data=Heart, subset=train) \n",
    "    pred2 <- predict(g2, Heart[test,])$class\n",
    "    pred3 <- predict(g3, Heart[test,])$class\n",
    "    RES2[i, 2] <- mean(pred2!=y.test) \n",
    "    RES2[i, 3] <- mean(pred3!=y.test) \n",
    "    \n",
    "    # Bayes Naive \n",
    "    g4 <- naiveBayes(AHD~., data=Heart, subset=train) \n",
    "    pred4 <- predict(g4, Heart[test,]) \n",
    "    RES2[i, 4] <- mean(pred4!= y.test) \n",
    "} \n",
    "\n",
    "apply(RES2, 2, mean) \n",
    "boxplot(cbind(RES1, RES2), col=2:8, boxwex=0.6, \n",
    "        names=c(\"unpruned tree\", \"pruned tree\", \"Logistic\", \"LDA\", \"QDA\", \"BayesNaive\"),\n",
    "        ylab=\"Classification Error Rate\")\n",
    "```\n",
    "\n",
    "![](Img/Tree15.png)\n",
    "- The missclassification rate of tree methods are higher than other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e48a99",
   "metadata": {},
   "source": [
    "# 6. Advantages and Disadvantages of Trees \n",
    "\n",
    "**Advantages** \n",
    "\n",
    "- Trees are very easy to explain to people. \n",
    "- Decision trees more closely mirror human decision-making than do the regression and other classification approaches. \n",
    "- Trees can be displayed graphically, and are easily interpreted. \n",
    "- Trees can easily handle qualitative predictors without the need to create dummy variables. \n",
    "\n",
    "**Disadvantages** \n",
    "\n",
    "- Trees do not have the same level of predictive accuracy as some of other regression and classification approaches. \n",
    "- Using **aggregating many decision trees** can improve the predictive performance of trees. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321.1px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
