{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a70521",
   "metadata": {},
   "source": [
    "# 1. Discriminant Analysis \n",
    "\n",
    "- Bayes Theorem : $Pr(Y=k|X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)}$\n",
    "- The density for $X$ in class $k$ : $f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}e^{-\\frac{(x-\\mu_k)^2}{2\\sigma_k^2}}$\n",
    "- The prior probability for class $k$ : $\\pi_k = Pr(Y=k)$\n",
    "- If the distribution of the $X$ is approximately normal, LDA and QDA is more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8549faa",
   "metadata": {},
   "source": [
    "## 1.1 LDA(Linear Discriminant Analysis) \n",
    "\n",
    "- Assume that $\\sigma = \\sigma_1 = \\sigma_2 = ... \\sigma_k$\n",
    "- $f_k(x)$ \n",
    "    - p = 1 : $f_k(x) \\to N(\\mu_k, \\sigma)$\n",
    "    - p > 1 : $f_k(x) \\to N(\\mu_k, \\sum)$\n",
    "- $p_k(x) =  \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu_k)^2}{2\\sigma^2}}}{\\sum_{l=1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu_l)^2}{2\\sigma^2}}}$\n",
    "- To classify at the value $X=x$, see which of the $p_k(x)$ is largest. \n",
    "- Because below term of $p_k(x)$ is same, we need to consider upper terms.\n",
    "    - Discriminant score : \n",
    "        - p = 1 : $\\sigma_k(x) = x\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k)$\n",
    "        - p > 1 : $\\sigma_k(x) = x^T\\sum^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\sum^{-1}\\mu_k + log(\\pi_k)$\n",
    "    - $\\sigma_k(x)$ is a linear function of $x$. \n",
    "- Decision boundary at $x = \\frac{\\hat{\\mu_1} + \\hat{\\mu_2}}{2}$\n",
    "- Need to estimate : $(\\mu_1, ..., \\mu_k), (\\pi_1, ..., \\pi_k), \\sum$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51e961",
   "metadata": {},
   "source": [
    "## 1.2 [Ex] Iris Data \n",
    "\n",
    "**Training model with LDA** \n",
    "\n",
    "```R\n",
    "# open the iris dataset \n",
    "data(iris)\n",
    "str(iris)\n",
    "summary(iris)\n",
    "plot(iris[, -5], col=as.numeric(iris$Species) + 1)\n",
    "\n",
    "# Apply LDA for iris data\n",
    "library(MASS)\n",
    "g <- lda(Species ~., data=iris)\n",
    "plot(g)\n",
    "plot(g, dimen=1)\n",
    "```\n",
    "\n",
    "![](Img/LDA02.png)\n",
    "\n",
    "**Compute Missclassification Rate for training sets** \n",
    "\n",
    "```R\n",
    "# Compute misclassification error for training sets\n",
    "pred <- predict(g)\n",
    "table(pred$class, iris$Species)\n",
    "mean(pred$class!=iris$Species)\n",
    "```\n",
    "\n",
    "![](Img/LDA01.png)\n",
    "\n",
    "**Calculate test error of validation set** \n",
    "\n",
    "```R\n",
    "# Randomly separate training sets and test sets\n",
    "set.seed(1234)\n",
    "tran <- sample(nrow(iris), size=floor(nrow(iris)*2/3))\n",
    "g <- lda(Species ~., data=iris, subset=tran)\n",
    "\n",
    "# Compute misclassification error for test sets\n",
    "pred <- predict(g, iris)$class[-tran]\n",
    "test <- iris$Species[-tran]\n",
    "table(pred, test)\n",
    "mean(pred!=test)\n",
    "\n",
    "# Posterior probability\n",
    "post <- predict(g, iris)$posterior[-tran,]\n",
    "post[1:10,]\n",
    "apply(post, 1, which.max)\n",
    "as.numeric(pred)\n",
    "``` \n",
    "![](Img/LDA03.png)\n",
    "\n",
    "- We can get inferred probability $\\hat{p}_k(x)$ : predict(g, iris)$posterior[-tran, ]\n",
    "\n",
    "**Performance comparison between LDA vs Multinomial Regression** \n",
    "\n",
    "```R\n",
    "library(nnet)\n",
    "set.seed(1234)\n",
    "K <- 100\n",
    "RES <- array(0, c(K, 2))\n",
    "for (i in 1:K) {\n",
    "    tran.num <- sample(nrow(iris), size=floor(nrow(iris)*2/3))\n",
    "    tran <- as.logical(rep(0, nrow(iris)))\n",
    "    tran[tran.num] <- TRUE\n",
    "    g1 <- lda(Species ~., data=iris, subset=tran)\n",
    "    g2 <- multinom(Species ~., data=iris, subset=tran, trace=FALSE)\n",
    "    pred1 <- predict(g1, iris[!tran,])$class\n",
    "    pred2 <- predict(g2, iris[!tran,])\n",
    "    RES[i, 1] <- mean(pred1!=iris$Species[!tran])\n",
    "    RES[i, 2] <- mean(pred2!=iris$Species[!tran])\n",
    "}\n",
    "apply(RES, 2, mean)\n",
    "\n",
    "# 0.0254 0.0436\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca409d",
   "metadata": {},
   "source": [
    "## 1.3 [Ex] Default Dataset \n",
    "\n",
    "```R\n",
    "# Importing library \n",
    "library(ISLR)\n",
    "data(Default)\n",
    "attach(Default)\n",
    "library(MASS)\n",
    "\n",
    "# Training model \n",
    "g <- lda(default~., data=Default)\n",
    "pred <- predict(g, default)\n",
    "table(pred$class, default)\n",
    "mean(pred$class!=default)\n",
    "```\n",
    "\n",
    "![](Img/LDA04.png)\n",
    "\n",
    "- False Positive rate : The fraction of negative examples that are classified as positive(0.22%)\n",
    "- False Negative rate : The fraction of positive examples that are classified as negative(76.2%)\n",
    "- If we classified the prior - always to class \"No\", then we would make 333/10000 errors, which is only 3.33%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fef05d",
   "metadata": {},
   "source": [
    "# 2. Assessment of the Performance of Classifier \n",
    "\n",
    "## 2.1 Two types of Missclassification errors \n",
    "\n",
    "- We can change the two error rates by changing the threshold from 0.5 to some other value in [0, 1].\n",
    "    - $\\hat{Pr}(Default = Yes|Balance, Student) \\ge \\alpha$\n",
    "    - $\\alpha$ is a threshold. \n",
    "    - If $\\alpha$ ↑, FN increase while FP decrease. \n",
    "    - If $\\alpha$ ↓, FN decrease while FP increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26913de",
   "metadata": {},
   "source": [
    "## 2.2 [Ex] Changes in errors along with Thresholds \n",
    "\n",
    "```R\n",
    "thresholds <- seq(0, 1, 0.01) \n",
    "res <- matrix(NA, length(thresholds), 3) \n",
    "\n",
    "# Compute overall error, false positive, false negatives\n",
    "for (i in 1:length(thresholds)) {\n",
    "    decision <- rep(\"No\", length(default))\n",
    "    decision[pred$posterior[,2] >= thresholds[i]] <- \"Yes\"\n",
    "    res[i, 1] <- mean(decision != default)\n",
    "    res[i, 2] <- mean(decision[default==\"No\"]==\"Yes\")\n",
    "    res[i, 3] <- mean(decision[default==\"Yes\"]==\"No\")\n",
    "}\n",
    "\n",
    "k <- 1:51\n",
    "matplot(thre[k], res[k,], col=c(1,\"orange\",4), lty=c(1,4,2), type=\"l\", xlab=\"Threshold\", ylab=\"Error Rate\", lwd=2)\n",
    "legend(\"top\", c(\"Overall Error\", \"False Positive\", \"False Negative\"), col=c(1,\"orange\",4), lty=c(1,4,2), cex=1.2)\n",
    "apply(res, 2, which.min)\n",
    "``` \n",
    "\n",
    "![](Img/Confusion1.png)\n",
    "\n",
    "- Overall error seems to decrease in every alpha, because there are only 22 FPs.  \n",
    "- However, it will increase slightly after the turning point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65800f68",
   "metadata": {},
   "source": [
    "## 2.3 Confusion Matrix \n",
    "\n",
    "![](Img/Confusion2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4f13d",
   "metadata": {},
   "source": [
    "## 2.4 Roc curve \n",
    "\n",
    "![](Img/ROC1.png)\n",
    "\n",
    "- Class-specific performance in medicine and biology : Sensitive(TPR), and specificity(TNR)\n",
    "- The ROC(Receiver Operating Characteristics) curve \n",
    "- ($\\alpha=1$, TPR=0, TNR=1) in left-lower point, ($\\alpha=0$, TPR=1, TNR=0) in right-upper point. \n",
    "- The overall performance of a classifier : AUC(The area under the ROC curve) \n",
    "    - Larger the AUC the better the classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0e232",
   "metadata": {},
   "source": [
    "## 2.5 [Ex] Roc curve \n",
    "\n",
    "\n",
    "**Way1 : Drawing ROC curve** \n",
    "\n",
    "```R\n",
    "# Prerequirisite\n",
    "library(ISLR)\n",
    "data(Default)\n",
    "attach(Default)\n",
    "library(MASS)\n",
    "\n",
    "# Train model \n",
    "g <- lda(default~., data=Default)\n",
    "pred <- predict(g, default)\n",
    "\n",
    "# Error grids\n",
    "thre <- seq(0,1,0.001)\n",
    "Sen <- Spe <- NULL\n",
    "RES <- matrix(NA, length(thre), 4)\n",
    "\n",
    "# Classification metrics \n",
    "colnames(RES) <- c(\"TP\", \"TN\", \"FP\", \"FN\")\n",
    "for (i in 1:length(thre)) {\n",
    "  decision <- rep(\"No\", length(default))\n",
    "  decision[pred$posterior[,2] >= thre[i]] <- \"Yes\"\n",
    "  Sen[i] <- mean(decision[default==\"Yes\"] == \"Yes\")\n",
    "  Spe[i] <- mean(decision[default==\"No\"] == \"No\")\n",
    "  RES[i,1] <- sum(decision[default==\"Yes\"] == \"Yes\")\n",
    "  RES[i,2] <- sum(decision[default==\"No\"] == \"No\")\n",
    "  RES[i,3] <- sum(decision==\"Yes\") - RES[i,1]\n",
    "  RES[i,4] <- sum(default==\"Yes\") - RES[i,1]\n",
    "}\n",
    "\n",
    "# Visualize ROc curve \n",
    "plot(1-Spe, Sen, type=\"b\", pch=20, xlab=\"False positive rate\",\n",
    "     col=\"darkblue\", ylab=\"True positive rate\", main=\"ROC Curve\")\n",
    "abline(0, 1, lty=3, col=\"gray\")\n",
    "```\n",
    "\n",
    "![](Img/ROC2.png)\n",
    "\n",
    "**Way2 : Drawing ROC curve**\n",
    "```R\n",
    "# Way 2 : Calculating TPR, TNR\n",
    "TPR <- RES[,1] / (RES[,1] + RES[,4])\n",
    "TNR <- RES[,2] / (RES[,2] + RES[,3])\n",
    "\n",
    "plot(1-TNR, TPR, type=\"b\", pch=20, xlab=\"False positive rate\",\n",
    "col=\"darkblue\", ylab=\"True positive rate\", main=\"ROC Curve\")\n",
    "abline(0, 1, lty=3, col=\"gray\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041966f2",
   "metadata": {},
   "source": [
    "## 2.6 [Ex] Roc curve with ROCR package\n",
    "\n",
    "```R\n",
    "library(ROCR)\n",
    "\n",
    "# Compute ROC curve\n",
    "label <- factor(default, levels=c(\"Yes\",\"No\"),\n",
    "labels=c(\"TRUE\",\"FALSE\"))\n",
    "preds <- prediction(pred$posterior[,2], label)\n",
    "perf <- performance(preds, \"tpr\", \"fpr\" )\n",
    "\n",
    "# Visualization \n",
    "plot(perf, lwd=4, col=\"darkblue\")\n",
    "abline(a=0, b=1, lty=2)\n",
    "slotNames(perf)\n",
    "\n",
    "k <- 1:100\n",
    "# X - axis values : FPR \n",
    "list(perf@x.name, perf@x.values[[1]][k])\n",
    "# Y - axis values : TPR \n",
    "list(perf@y.name, perf@y.values[[1]][k])\n",
    "# alpha - cutoffs \n",
    "list(perf@alpha.name, perf@alpha.values[[1]][k])\n",
    "\n",
    "# Compute AUC\n",
    "performance(preds, \"auc\")@y.values\n",
    "```\n",
    "\n",
    "![](Img/AUROC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525aae9",
   "metadata": {},
   "source": [
    "# 3. (QDA)Quardratic Discriminant Analysis \n",
    "\n",
    "- QDA assumes that each class has its own covariance matrix, $X ~ N(\\mu_k, \\sum _ k)$\n",
    "- LDA vs QDA\n",
    "    - Probability : $P(y_i=k|x)$\n",
    "    - X : $N(\\mu_k,\\sum)$ vs $N(\\mu_k, \\sum_k)$ \n",
    "    - Parameters : $\\mu_1, ..., \\mu_k$ vs $\\mu_1, ..., \\mu_k, \\sum_1, ..., \\sum_k$\n",
    "    - Num of grids : $PK + \\frac{P(P+1)}{2}$ vs $PK + K\\frac{P(P+1)}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e221ba0",
   "metadata": {},
   "source": [
    "## 3.1 LDA vs QDA \n",
    "\n",
    "**Classification Error Rate of LDA** \n",
    "\n",
    "```R\n",
    "# Import library \n",
    "library(ISLR) \n",
    "data(Default) \n",
    "attach(Default) \n",
    "library(MASS) \n",
    "\n",
    "# Train-test split\n",
    "set.seed(1234)\n",
    "n <- length(default) \n",
    "train <- sample(1:n, n*0.7) \n",
    "test <- setdiff(1:n, train) \n",
    "\n",
    "# Classification error rate of LDA \n",
    "g1 <- lda(default~., data=Default, subset=train)\n",
    "pred1 <- predict(g1, Default) \n",
    "table(pred1$class[test], Default$default[test]) \n",
    "mean(pred1$class[test]!=Default$default[test])\n",
    "``` \n",
    "\n",
    "![](Img/QDA1.png)\n",
    "\n",
    "**Classification Error Rate of QDA** \n",
    "```R\n",
    "# Classification error rate of QDA\n",
    "g2 <- qda(default~., data=Default, subset=train)\n",
    "pred2 <- predict(g2, Default)\n",
    "table(pred2$class[test], Default$default[test])\n",
    "mean(pred2$class[test]!=Default$default[test])\n",
    "```\n",
    "\n",
    "![](Img/QDA2.png)\n",
    "\n",
    "**AUC Score between LDA and QDA** \n",
    "\n",
    "```R\n",
    "# AUC comparison between LDA and QDA\n",
    "library(ROCR)\n",
    "label <- factor(default[test], levels=c(\"Yes\",\"No\"),\n",
    "                labels=c(\"TRUE\",\"FALSE\"))\n",
    "preds1 <- prediction(pred1$posterior[test,2], label)\n",
    "preds2 <- prediction(pred2$posterior[test,2], label)\n",
    "performance(preds1, \"auc\")@y.values\n",
    "performance(preds2, \"auc\")@y.values\n",
    "```\n",
    "\n",
    "![](Img/QDA3.png)\n",
    "\n",
    "- Performance of applying LDA works better than QDA\n",
    "\n",
    "**Simulation study of LDA and QDA iterating 100 times** \n",
    "\n",
    "```R\n",
    "# Simulation Study \n",
    "set.seed(123)\n",
    "N <- 100\n",
    "CER <- AUC <- matrix(NA, N, 2)\n",
    "for (i in 1:N) {\n",
    "  train <- sample(1:n, n*0.7)\n",
    "  test <- setdiff(1:n, train)\n",
    "  y.test <- Default$default[test]\n",
    "  g1 <- lda(default~., data=Default, subset=train)\n",
    "  g2 <- qda(default~., data=Default, subset=train)\n",
    "  pred1 <- predict(g1, Default)\n",
    "  pred2 <- predict(g2, Default)\n",
    "  CER[i,1] <- mean(pred1$class[test]!=y.test)\n",
    "  CER[i,2] <- mean(pred2$class[test]!=y.test)\n",
    "  label <- factor(default[test], levels=c(\"Yes\",\"No\"), labels=c(\"TRUE\",\"FALSE\"))\n",
    "  preds1 <- prediction(pred1$posterior[test,2], label)\n",
    "  preds2 <- prediction(pred2$posterior[test,2], label)\n",
    "  AUC[i,1] <- as.numeric(performance(preds1, \"auc\")@y.values)\n",
    "  AUC[i,2] <- as.numeric(performance(preds2, \"auc\")@y.values)\n",
    "}\n",
    "apply(CER, 2, mean)\n",
    "apply(AUC, 2, mean)\n",
    "```\n",
    "\n",
    "![](Img/QDA4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ef04b",
   "metadata": {},
   "source": [
    "# 4. Naive Bayes Method \n",
    "\n",
    "- Assumes that features are independent in each class. \n",
    "- Useful when p is large. \n",
    "- Gaussian naive Bayes assumes each $\\sum_k$ is diagonal. \n",
    "- Despite strong assumptions, NB method often produces good classification results. \n",
    "\n",
    "## 4.1 [Ex] Naive Bayes of Iris dataset \n",
    "\n",
    "\n",
    "**Calculate Train Error**\n",
    "\n",
    "```R\n",
    "# Import library and data\n",
    "data(iris)\n",
    "library(e1071) \n",
    "\n",
    "# Train model \n",
    "# g1 <- naiveBayes(Species ~ ., data=iris) \n",
    "g1 <- naiveBayes(iris[,-5], iris[,5])\n",
    "pred <- predict(g1, iris[,-5]) \n",
    "table(pred, iris[,5]) \n",
    "mean(pred!=iris$Species) \n",
    "```\n",
    "\n",
    "![](Img/NB1.png)\n",
    "\n",
    "**Validation Set** \n",
    "\n",
    "```R\n",
    "# Randomly separate training sets and test sets\n",
    "set.seed(1234)\n",
    "tran <- sample(nrow(iris), size=floor(nrow(iris)*2/3))\n",
    "\n",
    "# Compute misclassification error for test sets\n",
    "g2 <- naiveBayes(Species ~ ., data=iris, subset=tran)\n",
    "pred2 <- predict(g2, iris)[-tran]\n",
    "test <- iris$Species[-tran]\n",
    "table(pred2, test)\n",
    "mean(pred2!=test)\n",
    "```\n",
    "\n",
    "![](Img/NB2.png)\n",
    "\n",
    "## 4.2 [Ex] Naive Bayes of default dataset \n",
    "\n",
    "**Calculate Missclassifiaction Error Rate of Test-set**\n",
    "\n",
    "```R\n",
    "# Import dataset \n",
    "data(Default)\n",
    "\n",
    "# Train-test split \n",
    "set.seed(1234)\n",
    "n <- nrow(Default)\n",
    "train <- sample(1:n, n*0.7)\n",
    "test <- setdiff(1:n, train)\n",
    "\n",
    "# train model and calculate missclassification rate \n",
    "g3 <- naiveBayes(default ~ ., data=Default, subset=train)\n",
    "pred3 <- predict(g3, Default)[test]\n",
    "table(pred3, Default$default[test])\n",
    "mean(pred3!=Default$default[test])\n",
    "```\n",
    "\n",
    "![](Img/NB3.png)\n",
    "\n",
    "**AUC of Naive Bayes** \n",
    "\n",
    "```R\n",
    "# AUC of Naive Bayes \n",
    "library(ROCR)\n",
    "label <- factor(default[test], levels=c(\"Yes\",\"No\"), labels=c(\"TRUE\",\"FALSE\"))\n",
    "pred4 <- predict(g3, Default, type=\"raw\")\n",
    "preds <- prediction(pred4[test, 2], label)\n",
    "performance(preds, \"auc\")@y.values\n",
    "```\n",
    "\n",
    "- 0.9454898"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b7563",
   "metadata": {},
   "source": [
    "# 5. KNN(K-Nearest Neighbors)\n",
    "\n",
    "- Predict qualitative response using the Bayes classifier. \n",
    "- KNN classifier estimates the conditional distribution of \n",
    "    - $Pr(Y=j|X=x_0) = \\frac{1}{K}\\sum_{i \\in N_0} I(y_i=j)$\n",
    "    - $x_0$ : a test observation \n",
    "    - $N_0$ : a set of K points in the training data that are closest to $x_0$.\n",
    "    \n",
    "\n",
    "![](Img/KNN1.png) \n",
    "\n",
    "- KNN decision boundary(black) and The Bayesian decision boundary(purple). \n",
    "- The choice of $K$ has a drastic effect on the KNN classifier. \n",
    "- When K=1, the decision boundary is overfitting(low bias + high variance). \n",
    "- When K=100, the decision boundary is underfitting(high bias + low variance). \n",
    "- We need to find the best $K$ which optimizes the test error rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96020047",
   "metadata": {},
   "source": [
    "## 5.1 [Ex] Caran Insurance Data \n",
    "\n",
    "**Prepare dataset** \n",
    "\n",
    "```R\n",
    "# Importing library and data \n",
    "library(ISLR)\n",
    "data(Caravan)\n",
    "dim(Caravan)\n",
    "str(Caravan)\n",
    "attach(Caravan)\n",
    "\n",
    "# only 6% of people purchased caravan insurance.\n",
    "summary(Purchase)\n",
    "mean(Purchase==\"Yes\")\n",
    "```\n",
    "\n",
    "**Training logistic regression** \n",
    "\n",
    "```R\n",
    "# Logistic regression \n",
    "g0 <- glm(Purchase~., data=Caravan, family=\"binomial\")\n",
    "summary(g0)\n",
    "````\n",
    "\n",
    "**Training glmnet with CV** \n",
    "\n",
    "```R\n",
    "library(glmnet)\n",
    "y <- Purchase\n",
    "x <- as.matrix(Caravan[,-86])\n",
    "\n",
    "# glmnet with cross validation\n",
    "set.seed(123)\n",
    "g1.cv <- cv.glmnet(x, y, alpha=1, family=\"binomial\")\n",
    "plot(g1.cv)\n",
    "\n",
    "# Extract the value of lambda of model g1.cv \n",
    "g1.cv$lambda.min\n",
    "g1.cv$lambda.1se\n",
    "\n",
    "# Check coefficients \n",
    "coef1 <- coef(g1.cv, s=\"lambda.min\")\n",
    "coef2 <- coef(g1.cv, s=\"lambda.1se\")\n",
    "cbind(coef1, coef2)\n",
    "\n",
    "# Degree of freedom \n",
    "sum(coef1!=0)-1\n",
    "sum(coef2!=0)-1\n",
    "````\n",
    "\n",
    "![](Img/KNN2.png)\n",
    "\n",
    "- Degree of freedom \n",
    "    - lambda.min = 31\n",
    "    - lambda.1se = 8 \n",
    "    \n",
    "    \n",
    "**KNN methods** \n",
    "\n",
    "```R\n",
    "# Standardize data so that mean=0 and variance=1.\n",
    "X <- scale(Caravan[,-86])\n",
    "apply(Caravan[,1:5], 2, var)\n",
    "apply(X[,1:5], 2, var)\n",
    "\n",
    "# Separate training sets and test sets\n",
    "test <- 1:1000\n",
    "train.X <- X[-test, ]\n",
    "test.X <- X[test, ]\n",
    "train.Y <- Purchase[-test]\n",
    "test.Y <- Purchase[test]\n",
    "\n",
    "## Classification error rate of KNN\n",
    "set.seed(1)\n",
    "\n",
    "knn.pred <- knn(train.X, test.X, train.Y, k=1)\n",
    "mean(test.Y!=knn.pred)\n",
    "mean(test.Y!=\"No\")\n",
    "table(knn.pred, test.Y)\n",
    "\n",
    "knn.pred=knn(train.X, test.X, train.Y, k=3)\n",
    "table(knn.pred, test.Y)\n",
    "mean(test.Y!=knn.pred)\n",
    "\n",
    "knn.pred=knn(train.X, test.X, train.Y, k=5)\n",
    "table(knn.pred, test.Y)\n",
    "mean(test.Y!=knn.pred)\n",
    "\n",
    "knn.pred=knn(train.X, test.X, train.Y, k=10)\n",
    "table(knn.pred, test.Y)\n",
    "mean(test.Y!=knn.pred)\n",
    "```\n",
    "\n",
    "![](Img/KNN4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f15addb",
   "metadata": {},
   "source": [
    "## 5.2 [Ex] KNN with Hyperparameter Tuning of K based on Validation Set \n",
    "\n",
    "```R\n",
    "# Import dataset \n",
    "library(ISLR)\n",
    "data(Caravan)\n",
    "attach(Caravan)\n",
    "library(class)\n",
    "\n",
    "# Train-Test Splitting : Train-Validation-Test set \n",
    "set.seed(1234)\n",
    "n <- nrow(Caravan) \n",
    "s <- sample(rep(1:3, length=n))\n",
    "tran <- s==1\n",
    "valid <- s==2 \n",
    "test <- s==3 \n",
    "\n",
    "# Hyperparameter of K \n",
    "K = 100 \n",
    "\n",
    "# Train-Test Splitting : Train-Validation-Test set \n",
    "X <- scale(Caravan[,-86])\n",
    "y <- Caravan[,86]\n",
    "\n",
    "train.X <- X[tran,]\n",
    "valid.X <- X[valid,]\n",
    "test.X <- X[test,]\n",
    "train.y <- y[tran]\n",
    "valid.y <- y[valid]\n",
    "test.y <- y[test]\n",
    "\n",
    "# Calculate Missclassification Error rate of validation set \n",
    "miss <- rep(0, K)\n",
    "for (i in 1:K) { \n",
    "  knn.pred <- knn(train.X, valid.X, train.y, k=i)\n",
    "  miss[i] <- mean(valid.y != knn.pred)\n",
    "}\n",
    "miss\n",
    "wm <- which.min(miss)\n",
    "\n",
    "# Calculate Missclassifiaction Error rate of test set\n",
    "miss_test <- knn(train.X, test.X, train.y, k=wm)\n",
    "mean(test.y != miss_test)\n",
    "```\n",
    "\n",
    "- The optimized value of K is 8, and its missclassification error rate is 0.06134021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c89bb",
   "metadata": {},
   "source": [
    "## 5.3 [Ex] KNN Simulation Study \n",
    "\n",
    "\n",
    "```R\n",
    "# Import dataset \n",
    "library(ISLR)\n",
    "data(Caravan)\n",
    "attach(Caravan)\n",
    "library(class)\n",
    "\n",
    "# Simulation \n",
    "library(mnormt)\n",
    "set.seed(1010)\n",
    "sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)\n",
    "x.tran1 <- rmnorm(100, c(0, 0.8), sigma)\n",
    "x.tran2 <- rmnorm(100, c(0.8, 0), sigma)\n",
    "x.test1 <- rmnorm(3430, c(0, 0.8), sigma)\n",
    "x.test2 <- rmnorm(3430, c(0.8 ,0), sigma)\n",
    "x.tran <- rbind(x.tran1, x.tran2)\n",
    "x.test <- rbind(x.test1, x.test2)\n",
    "y.tran <- factor(rep(0:1, each=100))\n",
    "mn <- min(x.tran)\n",
    "mx <- max(x.tran)\n",
    "px1 <- seq(mn, mx, length.out=70)\n",
    "px2 <- seq(mn, mx, length.out=98)\n",
    "gd <- expand.grid(x=px1, y=px2)\n",
    "\n",
    "# Training model \n",
    "g1 <- knn(x.tran, gd, y.tran, k = 1, prob=TRUE)\n",
    "g2 <- knn(x.tran, gd, y.tran, k = 10, prob=TRUE)\n",
    "g3 <- knn(x.tran, gd, y.tran, k = 100, prob=TRUE)\n",
    "\n",
    "# Visualization of K=1 \n",
    "par(mfrow=c(1,3))\n",
    "prob1 <- attr(g1, \"prob\")\n",
    "prob1 <- ifelse(g1==\"1\", prob1, 1-prob1)\n",
    "pp1 <- matrix(prob1, length(px1), length(px2))\n",
    "contour(px1, px2, pp1, levels=0.5, labels=\"\", xlab=\"\", ylab=\"\",\n",
    "        main=\"KNN: K=1\", axes=FALSE)\n",
    "points(x.tran, col=ifelse(y.tran==1, \"cornflowerblue\", \"coral\"))\n",
    "co1 <- ifelse(pp1>0.5, \"cornflowerblue\", \"coral\")\n",
    "points(gd, pch=\".\", cex=1.2, col=co1)\n",
    "box()\n",
    "\n",
    "# Visualization of K=10\n",
    "prob2 <- attr(g2, \"prob\")\n",
    "prob2 <- ifelse(g2==\"1\", prob2, 1-prob2)\n",
    "pp2 <- matrix(prob2, length(px1), length(px2))\n",
    "contour(px1, px2, pp2, levels=0.5, labels=\"\", xlab=\"\", ylab=\"\",\n",
    "        main=\"KNN: K=10\", axes=FALSE)\n",
    "points(x.tran, col=ifelse(y.tran==1, \"cornflowerblue\", \"coral\"))\n",
    "co2 <- ifelse(pp2>0.5, \"cornflowerblue\", \"coral\")\n",
    "points(gd, pch=\".\", cex=1.2, col=co2)\n",
    "box()\n",
    "\n",
    "# Visualization of K = 100 \n",
    "prob3 <- attr(g3, \"prob\")\n",
    "prob3 <- ifelse(g3==\"1\", prob3, 1-prob3)\n",
    "pp3 <- matrix(prob3, length(px1), length(px2))\n",
    "contour(px1, px2, pp3, levels=0.5, labels=\"\", xlab=\"\", ylab=\"\",\n",
    "        main=\"KNN: K=100\", axes=FALSE)\n",
    "points(x.tran, col=ifelse(y.tran==1, \"cornflowerblue\", \"coral\"))\n",
    "co3 <- ifelse(pp3>0.5, \"cornflowerblue\", \"coral\")\n",
    "points(gd, pch=\".\", cex=1.2, col=co3)\n",
    "box()\n",
    "```\n",
    "\n",
    "When the graph according to the value of K is checked, it can be seen that the decision boundary is overfitting when K = 1. If K = 100, the decision boundary is briefly drawn. It can be seen that K = 10 forms an appropriate decision boundary. \n",
    "\n",
    "![](Img/KNN5.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "319.967px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
