---
title: "HW1_201724570"
author: "Jeong Seok Gyu"
date: '2022-10-04'
output: 
  html_document:
    latex_engine: xelatex
mainfont: NanumGothic
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```
Student information

- Student name : 정석규
- Major : 전기공학과 
- Student id : 201724570
```

## Table of Contents 

1. [Introduction](#01 Introduction)
2. [Problem 1](#02 Problem 1)
3. [Problem 2](#03 Problem 2)
4. [Problem 3](#04 Problem 3)
5. [Problem 4](#05 Problem 4) 
6. [Problem 5](#06 Problem 5) 
7. [Problem 6](#07 Problem 6)

---

<a name="01 Introduction"></a>

# Introduction

Open the data set Boston in the $R$ package $MASS$. The data information is available with ?Boston. It has a total of $n = 506$ observations with 14 variables, where the variable $crim$ is considered as a response and the other 13 variables ar considered as predictors. So, you can make the predictor $x$ and the response $y$ using the following R codes.

```
data(Boston)
y <- Boston[, 1]
x <- Boston[, -1]
```

We want to find the best subset among 13 predictors associated with the response $y$. In order to find the best model, you have to consider a total of $2^{13} - 1 = 8,191$ models. Let us define the log-likelihood function $l(\theta)$, $AIC$(Akaike information criterion) and $BIC$(Bayesian information criterion) as 

$$l(\theta) = \sum_{i=1}^{n} \log{(\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y_i - \beta_0 - x_i^T \beta)^2}{2\sigma^2}))}$$
$$AIC(\theta) = -2 l(\theta) + 2d$$
$$BIC(\theta) = -2l(\theta) + d \log{n}$$
respectively. The parameter vector $\theta = (\beta_0, \beta, \sigma)$ and $d$ is the number of regression coefficient in the model. Note that $d \ge 1$ since an intercept parameter $\beta_0$ is always in the model. 

```{r warning=FALSE, message=FALSE}
# ---------- Introduction ---------- # 
# Importing packages 
## install.packages('MASS')
## install.packages('leaps')
## install.packages('glmnet')￣
library(glmnet)
library(MASS)
library(leaps)
library(dplyr)

# Importing Datasets 
data(Boston) 
dim(Boston)  

# Make predictor x and the response y 
y <- Boston[, 1]
x <- Boston[, -1]
```

```{r}
# Predefined function log-likelihood, AIC and BIC. 

# 1. Predicting function for best subsets. 
predict.regsubsets <- function(x, coefi, ...) {
  mat.x <- model.matrix(~., x)
  xvars <- names(coefi)
  pred <- mat.x[, xvars] %*% coefi
  return(pred)
}

# 2. log-likelihood function 
likelihood <- function(x, y, coefi, sigma){
  pred <- predict.regsubsets(x, coefi)
  exp_term = exp(-(y - pred)^2 / (2 * sigma^2))
  res <- sum(log(exp_term / (sqrt(2 * pi) * sigma)))
  return(res)
}

# 3. AIC(Akaike information criterion) 
aic <- function(ltheta, d) { 
  res <- (-2 * ltheta) + (2 * d)  
  return(res)
}

# 4. BIC(Bayesian information criterion) 
bic <- function(ltheta, d, n) { 
  res <- (-2 * ltheta) + (d * log(n))  
  return(res)
}
```

```{r}
# Pre-define most used variables 
n <- nrow(x) 
ybar <- mean(y) 
```

--- 

<a name="02 Problem 1"></a>

# Problem 1

With the ordinary least square estimate $(\hat{\beta_0^{ols}}, \hat{\beta^{ols}})$ and the plug-in estimate $\hat{\sigma_1} = \sqrt{\frac{1}{n-1}\sum_{i=1}^n(y_i - \bar{y})^2}$, find the best subset model that can minimize $AIC$ and $BIC$, respectively. For each best subset, clearly specify which variables are included in the final model along with the numerical values of $AIC$ and $BIC$. 

```{r warning=FALSE, message=FALSE}
# ---------- Problem1 ---------- # 
# Training model with the ordinary square estimate 
# Workflows of problem 1
# 1. Fit all (p=13, k) models that contains exactly k predictors. 
# 2. Pick the best among these (p=13, k) models, and call it M_k. (We will do this by using regsubsets function.)
# 3. Select a single best model from among M_0, M_1, ... M_13 using AIC and BIC. 
#   3.1 Calculate coefi, d, sigma1 from each k. 
#   3.2 Calculate log-likelihood values from likelihood function. 
# 4. Evaluate each model using AIC and BIC and store the results in metrics1. 

# \sigma 1 
sigma_formula1 <- function(y, ybar, n) {
  res <- sqrt(sum((y - ybar)^2) / (n - 1))
  return(res)
}

# train the best subset selection model
g1 <- regsubsets(x, y, nvmax=13, nbest=1) 

# model evaluation
metrics1 <- matrix(0, 13, 2)
for (i in 1:13) {
  coefi <- coef(g1, id=i)
  d <- length(coefi)
  sigma1 <- sigma_formula1(y, ybar, n)
  ltheta <- likelihood(x, y, coefi, sigma1)
  metrics1[i, 1] <- aic(ltheta, d)
  metrics1[i, 2] <- bic(ltheta, d, n)
}

# find the smallest number of variables based on AIC and BIC
colnames(metrics1) <- c('AIC', 'BIC')
metrics1
wm.a1 <- which.min(metrics1[, 1])
wm.b1 <- which.min(metrics1[, 2]) 

# Visualization of metrics plot 
par(mfrow=c(1,2))
plot(1:13, metrics1[, 1], type="b", pch=20, xlab="Number of Predictors", ylab="AIC")
points(wm.a1, metrics1[wm.a1, 1], pch="o", col="blue", cex=2)
plot(1:13, metrics1[, 2], type="b", pch=20, xlab="Number of Predictors", ylab="BIC")
points(wm.b1, metrics1[wm.b1, 2], pch="o", col="blue", cex=2)

# Print Results 
cat("The variables included in final model based on AIC : ", names(coef(g1, id=wm.a1)[-1]), sep="\t")
cat("The number of variables included in final model based on AIC : ", length(names(coef(g1, id=wm.a1))) - 1)
cat("The value of its AIC : ", metrics1[wm.a1, 1])

cat("The variables included in final model based on BIC : ", names(coef(g1, id=wm.b1))[-1], sep="\t")
cat("The number of variables included in final model based on BIC : ", length(names(coef(g1, id=wm.b1))) - 1)
cat("The value of its BIC : ", metrics1[wm.b1, 2])
```

--- 

<a name="03 Problem 2"></a>

# Problem 2

Repeat Q1, replacing $\hat{\sigma_1}$ with $\hat{\sigma_2} = \sqrt{\frac{1}{n-d}\sum_{i=1}^n(y_i - \hat{\beta_0^{ols}} - x_i^T \hat{\beta^{ols}}^2)}$ 

```{r}
# ---------- Problem2 ---------- # 
# Repeat Q1 replacing sigma1 with sigma2. 
# Workflows of solving problem 2.
# 1. Define sigma2 formula within sigma_formula2 function. 
# 2. Repeat the same steps what we done in Problem1.

# \sigma 2
sigma_formula2 <- function(x, y, coefi, n, d) {
  pred <- predict.regsubsets(x, coefi)
  res <- sqrt(sum((y- pred)^2) / (n - d))
  return(res)
}

# train the best subset selection model
g2 <- regsubsets(x, y, nvmax=13, nbest=1) 

# model evaluation : this tasks will be same without applying different sigma 
metrics2 <- matrix(0, 13, 2)
for (i in 1:13) {
  coefi <- coef(g2, id=i)
  d <- length(coefi) 
  sigma2 <- sigma_formula2(x, y, coefi, n, d)
  ltheta <- likelihood(x, y, coefi, sigma2)
  metrics2[i, 1] <- aic(ltheta, d)
  metrics2[i, 2] <- bic(ltheta, d, n)
}

# find the smallest number of variables based on AIC and BIC
colnames(metrics2) <- c("AIC", "BIC") 
metrics2
wm.a2 <- which.min(metrics2[, 1])
wm.b2 <- which.min(metrics2[, 2]) 

# Visualization of metrics plot 
par(mfrow=c(1,2))
plot(1:13, metrics2[, 1], type="b", pch=20, xlab="Number of Predictors", ylab="AIC")
points(wm.a2, metrics2[wm.a2, 1], pch="o", col="blue", cex=2)
plot(1:13, metrics2[, 2], type="b", pch=20, xlab="Number of Predictors", ylab="BIC")
points(wm.b2, metrics2[wm.b2, 2], pch="o", col="blue", cex=2)

# Print Results 
cat("The variables included in final model based on AIC : ", names(coef(g2, id=wm.a2)[-1]), sep="\t")
cat("The number of variables included in final model based on AIC : ", length(names(coef(g2, id=wm.a2))) - 1)
cat("The value of its AIC : ", metrics2[wm.a2, 1])

cat("The variables included in final model based on BIC : ", names(coef(g2, id=wm.b2))[-1], sep="\t")
cat("The number of variables included in final model based on BIC : ", length(names(coef(g2, id=wm.b2))) - 1)
cat("The value of its BIC : ", metrics1[wm.b2, 2])
```

--- 

<a name="04 Problem 3"></a>

# Problem 3

Fit lasso with all of 13 predictors to find the best subset among 1,000 different $\lambda$ values. You don't need to consider 8.191 models here. Use the following R code to generate the $\lambda$ values. 

```
lambda <- 10^seq(0.8, -3, length=1000)
```

With the lasso estimate $(\hat{\beta_0^{lasso}}, \hat{\beta^{lasso}})$ and the plug-in estimator $\hat{\sigma_1}$, find the best subset models that can minimize the $AIC$ and $BIC$, respectively. For each best subset, clearly specify which variables are included in the final model along with the numerical values of $AIC$ and $BIC$. 

```{r}
# ---------- Problem3 ---------- # 
# In this problem, we need to apply hyperparameter thunig using lambda grid. 
# Workflows of Problem 3 
# 1. Define lambda grid.
# 2. Train model in lambda grid. 
# 3. Repeat the steps through the lambda grid. 
#   3.1 Calculate degree of freedom and prediction from each lambda. 
#   3.2 Calculate AIC and BIC from prediction and store results into metrics3.
# 4. Evaluate models.

# Define new function for calculating log-likelihood 
likelihood2 <- function(y, pred, sigma){
  exp_term = exp(-(y - pred)^2 / (2 * sigma^2))
  res <- sum(log(exp_term / (sqrt(2 * pi) * sigma)))
  return(res)
}

# Set hyper parameters grid lambda 
lambda <- 10^seq(0.8, -3, length=1000)

# training model 
g3 <- glmnet(x, y, alpha=1, lambda=lambda)

# model evaluation
metrics3 <- matrix(0, length(g3$lambda), 2)
for (i in 1:length(g3$lambda)) {
  d <- g3$df[i] + 1
  pred <- predict(g3, s=g3$lambda[i], newx=model.matrix(~., x)[,-1])
  ltheta <- likelihood2(y, pred, sigma1)
  metrics3[i, 1] <- aic(ltheta, d)
  metrics3[i, 2] <- bic(ltheta, d, n)
}

# find the smallest number of variables based on AIC and BIC
colnames(metrics3) <- c('AIC', 'BIC')
metrics3[sample(1:800, 10, replace=FALSE),]
wm.a3 <- which.min(metrics3[, 1])
wm.b3 <- which.min(metrics3[, 2]) 

# Visualization of metrics plot 
par(mfrow=c(1,2))
plot(log(g3$lambda), metrics3[, 1], type="b", pch=20, xlab="log(lambda)", ylab="AIC")
points(log(g3$lambda)[wm.a3], metrics3[wm.a3, 1], pch="o", col="blue", cex=2)
plot(log(g3$lambda), metrics3[, 2], type="b", pch=20, xlab="log(lambda)", ylab="BIC")
points(log(g3$lambda)[wm.b3], metrics3[wm.b3, 2], pch="o", col="blue", cex=2)

# Print Results 
coef3a <- coef(g3, s=g3$lambda[wm.a3])
cat("The variables included in final model based on AIC : ", coef3a@Dimnames[[1]][which(coef3a != 0)][-1], sep="\t")
cat("The number of variables included in final model based on AIC : ", length(coef3a@Dimnames[[1]][which(coef3a != 0)][-1]))
cat("The value of its AIC : ", metrics3[wm.a3, 1])

coef3b <- coef(g3, s=g3$lambda[wm.b3])
cat("The variables included in final model based on BIC : ", coef3b@Dimnames[[1]][which(coef3b != 0)][-1], sep="\t")
cat("The number of variables included in final model based on BIC : ", length(coef3b@Dimnames[[1]][which(coef3b != 0)][-1]))
cat("The value of its BIC : ", metrics3[wm.b3, 2])
```

--- 

<a name="05 Problem 4"></a>

# Problem 4 

Repeat Q3, replacing $\hat{\sigma_1}$ with $\hat{\sigma_2}$

```{r}
# ---------- Problem4 ---------- # 
# In this problem, we need to replace sigma1 with sigma2. 
# However, to calculate \beta_{ols}, we need to calculate once more to get sigma2. 
# Workflows of problem 4 
# 1. Define lambda grid
# 2. Train model in lambda grid(using glmnet function)
# 3. Repeat the steps through lambda grid 
#   3.1 Calculate d, xvars, and prediction from each lambda. 
#   3.2 With selected colu￣mns from lasso models, calculate \beta_{ols} (using lm function)
#   3.3 Calculate sigma2 from coefficients which we got from lm function. 
#   3.4 Calculate AIC and BIC from prediction and store results into metrics4.
# 4. Evaluate models.

sigma2_lasso <- function(d, xvars) {
  if (length(xvars) == 0) {
    # If there is no selected variables from glmnet, return ols_pred as ybar.
    # Then the result will be sigma1. 
    return(sigma_formula1(y, ybar, n))
  } else {
    # This will calculate after we select variables from lasso.
    mat.x <- x[ , xvars]
    ols <- lm(y~as.matrix(mat.x))
    ols_pred <- as.matrix(cbind(1, mat.x)) %*% ols$coefficients
  }
  res <- sqrt(sum((y- ols_pred)^2) / (n - d))
  return(res)
}
  
# Set hyper parameters grid lambda 
lambda <- 10^seq(0.8, -3, length=1000)

# Train model 
g4 <- glmnet(x, y, alpha=1, lambda=lambda)

# model evaluation
metrics4 <- matrix(0, length(g4$lambda), 2)
for (i in 1:length(g4$lambda)) {
  d <- g4$df[i] + 1 
  coefi <- coef(g4, s=g4$lambda[i])
  xvars <- coefi@Dimnames[[1]][which(coefi != 0)][-1]
  sigma2 <- sigma2_lasso(d, xvars)
  pred <- predict(g4, s=g4$lambda[i], newx=model.matrix(~., x)[,-1])
  ltheta <- likelihood2(y, pred, sigma2)
  metrics4[i, 1] <- aic(ltheta, d)
  metrics4[i, 2] <- bic(ltheta, d, n)
}

# find the smallest number of variables based on AIC and BIC
colnames(metrics4) <- c('AIC', 'BIC')
metrics4[sample(1:800, 10, replace=FALSE),]
wm.a4 <- which.min(metrics4[, 1])
wm.b4 <- which.min(metrics4[, 2]) 

# Visualization of metrics plot 
par(mfrow=c(1,2))
plot(log(g4$lambda), metrics4[, 1], type="b", pch=20, xlab="log(lambda)", ylab="AIC")
points(log(g4$lambda)[wm.a4], metrics4[wm.a4, 1], pch="o", col="blue", cex=2)
plot(log(g4$lambda), metrics4[, 2], type="b", pch=20, xlab="log(lambda)", ylab="BIC")
points(log(g4$lambda)[wm.b4], metrics4[wm.b4, 2], pch="o", col="blue", cex=2)

# Print Results 
coef4a <- coef(g4, s=g4$lambda[wm.a4])
cat("The variables included in final model based on AIC : ", coef4a@Dimnames[[1]][which(coef4a != 0)][-1], sep = "\t")
cat("The number of variables included in final model based on AIC : ", length(coef4a@Dimnames[[1]][which(coef4a != 0)][-1]))
cat("The value of its AIC : ", metrics4[wm.a4, 1])

coef4b <- coef(g4, s=g4$lambda[wm.b4])
cat("The variables included in final model based on BIC : ", coef4b@Dimnames[[1]][which(coef4b != 0)][-1], sep="\t")
cat("The number of variables included in final model based on BIC : ", length(coef4b@Dimnames[[1]][which(coef4b != 0)][-1]))
cat("The value of its BIC : ", metrics4[wm.b4, 2])
```

--- 

<a name="06 Problem 5"></a>

# Problem 5 

Repeat Q3, replacing $\hat{\sigma_1}$ with $\hat{\sigma_3} = \sqrt{\frac{1}{n-d}\sum_{i=1}^n(y_i - \hat{\beta_0^{lasso}} - x_i^T \hat{\beta^{lasso}}^2)}$

```{r}
# ---------- Problem5 ----------
# Repate Q3, replacing sigma1 with sigma3 which is calculated from \beta_{lasso}
# Workflows of Problem 5
# 1. Define new sigma function called sigma_formula3
# 2. Define hyperparameter grid lambda. 
# 3. Train model in lambda grid(using glmnet function)
# 4. Repeat the steps through lambda grid 
#   4.1 Calculate d and prediction from each lambda. 
#   4.2 Calculate sigma3 using sigma_formula3 function. 
#   4.3 Calculate AIC and BIC from prediction and store results into metrics5.
# 4. Evaluate models.

# \sigma 3
sigma_formula3 <- function(y, pred, n, d) {
  res <- sqrt(sum((y- pred)^2) / (n - d))
  return(res)
}

# Set hyper parameters grid lambda 
lambda <- 10^seq(0.8, -3, length=1000)

# Train model 
g5 <- glmnet(x, y, alpha=1, lambda=lambda)

# model evaluation
metrics5 <- matrix(0, length(g5$lambda), 2)
for (i in 1:length(g5$lambda)) {
  d <- g5$df[i] + 1 
  pred <- predict(g5, s=g5$lambda[i], newx=model.matrix(~., x)[,-1])
  sigma3 <- sigma_formula3(y, pred, n, d)
  ltheta <- likelihood2(y, pred, sigma3)
  metrics5[i, 1] <- aic(ltheta, d)
  metrics5[i, 2] <- bic(ltheta, d, n)
}

# find the smallest number of variables based on AIC and BIC
colnames(metrics5) <- c('AIC', 'BIC')
metrics5[sample(1:800, 10, replace=FALSE),]
wm.a5 <- which.min(metrics5[, 1])
wm.b5 <- which.min(metrics5[, 2]) 

# Visualization of metrics plot 
par(mfrow=c(1,2))
plot(log(g5$lambda), metrics5[, 1], type="b", pch=20, xlab="log(lamda)", ylab="AIC")
points(log(g5$lambda)[wm.a5], metrics5[wm.a5, 1], pch="o", col="blue", cex=2)
plot(log(g5$lambda), metrics5[, 2], type="b", pch=20, xlab="log(lambda)", ylab="BIC")
points(log(g5$lambda)[wm.b5], metrics5[wm.b5, 2], pch="o", col="blue", cex=2)

# Print Results 
coef5a <- coef(g5, s=g5$lambda[wm.a5])
cat("The variables included in final model based on AIC : ", coef5a@Dimnames[[1]][which(coef5a != 0)][-1], sep="\t")
cat("The number of variables included in final model based on AIC : ", length(coef5a@Dimnames[[1]][which(coef5a != 0)][-1]))
cat("The value of its AIC : ", metrics5[wm.a5, 1])

coef5b <- coef(g5, s=g5$lambda[wm.b5])
cat("The variables included in final model based on BIC : ", coef5b@Dimnames[[1]][which(coef5b != 0)][-1], sep="\t")
cat("The number of variables included in final model based on BIC : ", length(coef5b@Dimnames[[1]][which(coef5b != 0)][-1]))
cat("The value of its BIC : ", metrics5[wm.b5, 2])
```

---

<a name="07 Problem 6"></a>

# Problem 6 

Randomly separate a training set (tran) and a test set (test), using the following R code 

```
set.seed(4321) 
tran <- sample(nrow(x), 400)
test <- setdiff(1:nrow(x), tran) 
```

Find the best subset based on the training set, i.e., $n = 400$. In order to find the best subset, you should consider 5 different ways from Q1 to Q5. Since each question requires to find the best subset based on both $AIC$ and $BIC$, you actually have 10 different ways to find the best subset. Let us denote them by $M_1, M_2, ..., M_{10}$. Note that the best subsets from 10 different ways can be overlapped. After you find 10 best subsets from the training set, compute the test errors using $\frac{1}{106}\sum_{i=1}^{106}(y_i - \hat{\beta_0^{ols}} - x_i^T \hat{\beta^{ols}})^2$ for each subset. For computation of the test error, $(\hat{\beta_0^{ols}},\hat{\beta^{ols}})$ should be estimated from the training set while $i = 1, ... , 106$ is an index of the test set. Finally, provide the variable selection result from the training set and the test error (TE) for each subset using the following table, 

||zn|indus|chas|...|lstat|medv|TE|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|$M_1 (Q1/AIC)$|0/1|0/1|0/1|||||
|$M_2 (Q2/BIC)$||
| ... ||||||||
|$M_9 (Q5/AIC)$||||||||
|$N_{10} (Q5/BIC)$||||||||||

In the table, write '1' if the corresponding variable is included in the model, otherwise '0'. Who is winner?

```{r}
# ---------- Problem 6 ---------- # 
# Workflows of problem 6
# 1. Randomly separate a training set and test set. 
# 2. Declare result matrix called res and initialize with 0. 
# 3. For each training set, repeat all steps what we have done in problem 1 ~ 5. 
# 4. Update the value of selected variables(columns) for each model(rows) to 1. 
# 5. Calculate training error TE using MAE. 
# 6. Check the result and find the winner. 


# Randomly separate a training set and test set
set.seed(4321)
tran <- sample(n, 400) 
test <- setdiff(1:nrow(x), tran) 

# Define function for finding 10 best subsets from the training set. 
# This will return \beta_{ols} and selected_variables
find_best_model <- function(x, y, model_numbers) {
  # if : regsubsets function for M_1 ~ M_4
  # else : glmnet function for M_5 ~ M10
  if (model_numbers <= 4) {
    # train model
    g <- regsubsets(x, y, nvmax=13, nbest=1)
    
    # model evaluation
    metrics <- matrix(0, 13, 1) 
    for (i in 1:13) {
      coefi <- coef(g, id=i) 
      d <- length(coefi) 
      # if : evaluation based on sigma1
      # else : evaluation based on sigma2
      if (model_numbers <= 2) {
        sigma1 <- sigma_formula1(y, ybar, n) 
        ltheta <- likelihood(x, y, coefi, sigma1)
      } else { 
        sigma2 <- sigma_formula2(x, y, coefi, n, d)
        ltheta <- likelihood(x, y, coefi, sigma2)
      }
      # if : evaluation based on AIC 
      # else : evaluation based on BIC 
      if (model_numbers %% 2 != 0) { 
        metrics[i, 1] <- aic(ltheta, d)
      } else { 
        metrics[i, 1] <- bic(ltheta, d, n)
      }
    }
    # Find the smallest number of variables and return results 
    wm <- which.min(metrics) 
    min_vars <- names(coef(g, id=wm)[-1])
    min_coef <- coef(g, id=wm)
    return(list(min_vars, min_coef))
  } else { 
    lambda <- 10^seq(0.8, -3, length=1000)
    g <- glmnet(x, y, alpha=1, lambda=lambda)
    
    # model evaluation
    metrics <- matrix(0, length(g$lambda), 1)
    for (i in 1:length(g$lambda)) {
      d <- g$df[i] + 1 
      pred <- predict(g, s=g$lambda[i], newx=model.matrix(~., x)[,-1])
      # if : evaluation based on sigma1
      # elif : evaluation based on sigma2
      # else : evaluation based on sigma3
      if (model_numbers <= 6) {
        sigma1 <- sigma_formula1(y, ybar, n) 
        ltheta <- likelihood2(y, pred, sigma1)
      } else if (model_numbers <= 8) {
        xvars <- rownames(coef(g, s=g$lambda[i]))[-1]
        sigma2 <- sigma2_lasso(d, xvars) 
        ltheta <- likelihood2(y, pred, sigma2)
      } else {
        sigma3 <- sigma_formula3(y, pred, n, d)
        ltheta <- likelihood2(y, pred, sigma3)
      }
      # if : evaluation based on AIC 
      # else : evaluation based on BIC 
      if (model_numbers %% 2 != 0) { 
        metrics[i, 1] <- aic(ltheta, d)
      } else { 
        metrics[i, 1] <- bic(ltheta, d, n)
      }
    }
    # Find the smallest number of variables and return results 
    wm <- which.min(metrics) 
    coefm <- coef(g, s=g$lambda[wm])
    min_vars <- coefm@Dimnames[[1]][which(coefm != 0)][-1]
    min_coef <- coefm
    return(list(min_vars, min_coef))
  }
}

# Define function for calculuting test error based on MSE 
calculate_test_error <- function(x, y, xvars, coef_, model_number) {
  # if : Calculate test error based on regsubsets
  # else : Calculate test error based on glmnet 
  if (model_number <= 4) {
    pred <- model.matrix(~., test_X[, xvars]) %*% coef_
  } else { 
    pred <- model.matrix(~., test_X) %*% coef_
  }
  test_error <- mean((test_y - pred)^2)
  return(test_error)
}

# Declare result matrix (10 X 14)
res <- matrix(0, 10, ncol(x) + 1)
colnames(res) <- c(colnames(x), 'TE') 
rownames(res) <- c('Q1-AIC', 'Q1-BIC', 'Q2-AIC', 'Q2-BIC', 'Q3-AIC', 'Q3-BIC', 'Q4-AIC', 'Q4-BIC', 'Q5-AIC', 'Q5-BIC')

for (i in 1:10) { 
  train_X <- x[tran, ]
  train_y <- y[tran]
  test_X <- x[test,]
  test_y <- y[test]
  
  # Find the best model from each problems 
  train_result <- find_best_model(train_X, train_y, i)
  xvars <- as.vector(train_result[[1]])
  # Calculate Test Error based on MSE and selected variables
  res[i, 'TE'] <- calculate_test_error(test_X, test_y, xvars, train_result[[2]], i)
  res[i , xvars] <- 1
}

# Check the table
res

# Visualization result 
wm.t <- which.min(res[ , 'TE'])
plot(factor(rownames(res)), res[ , 'TE'], type="b", pch=20, cex=0.2, xlab="Models", ylab="Test Error")
points(factor(rownames(res))[wm.t], res[wm.t, 'TE'], pch="x", col="red", cex=2)
```