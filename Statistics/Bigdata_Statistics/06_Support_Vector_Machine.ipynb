{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64716abd",
   "metadata": {},
   "source": [
    "# 1. Support Vector Machine \n",
    "\n",
    "- Find a plane that separate the classes in feature space.\n",
    "- Soften what we mean by separates and enrich and enlarge the feature space so that separation is possible \n",
    "- Three methods \n",
    "    - Maximum margin classifier \n",
    "    - Support vector classifier(SVC) \n",
    "    - Support vector machine(SVM) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdcdf2",
   "metadata": {},
   "source": [
    "## 1.1 Hyperplane \n",
    "\n",
    "- A **hyperplane** in p dimensions is a flat affine subspace of dimension p-1. \n",
    "- General equation for a hyper plane : $\\beta_0 + \\beta_1 X_1 + ... \\beta_p X_p = 0$\n",
    "    - If $X = (X_1, ..., X_p)^T$ satisfies above, then $X$ lies on the hyper plane.\n",
    "    - If $X = (X_1, ..., X_p)^T$ does not satify above, then $X$ lies to one side of the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e5166",
   "metadata": {},
   "source": [
    "## 1.2 Separating Hyperplanes \n",
    "\n",
    "- If $f(x) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p$, then $f(x) > 0$ for points on one side of thehyperplane, and $f(x) < 0$ for points on the other. \n",
    "- If a separating hyperplane exists, we can use it to construct a very natural classifier. \n",
    "- For a test observation $x^{*}$ : $f(x^*) = \\beta_0 + \\beta_1 x_1^* + ... + \\beta_p x_p^*$\n",
    "    - If $f(x^*) > 0$, we assign the test observation to class 1. \n",
    "    - If $f(x^*) < 0$, we assign the test observation to class 2 \n",
    "- If $|f(x^*)|$ is relatively large, the class assignment is confident \n",
    "- If $|f(x^*)|$ is relatively small, the class assignment is less confident "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cdd8a",
   "metadata": {},
   "source": [
    "# 2. Maximal Margin Classifier \n",
    "\n",
    "- The **maximal margin hyperplane** is the separating hyperplane that is farthest from the training observations. \n",
    "- Among all separating hyperplane, find the one that makes the biggest gap or margin between the two classes. \n",
    "- Constrained optimization problem : \n",
    "    - maximize M subject to \n",
    "- The function svm() in an R package e1071 solves this problem efficiently. \n",
    "\n",
    "<img src=\"Img/SVM1.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de4a825",
   "metadata": {},
   "source": [
    "## 2.1 The Non-separable Case\n",
    "\n",
    "- The maximal margin classifier is a very natural way to perform classification. \n",
    "- However, in many cases no separating hyperplane exists, and so there is no maximal margin classifier. \n",
    "- If a separating hyperplane doesn't exist, there is no solution to M in the optimization problem. \n",
    "- However, we can develop a hyperplane that almost separates the classes, using a so-called soft margin. \n",
    "- The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f185938",
   "metadata": {},
   "source": [
    "# 3. Support Vector Classifier \n",
    "\n",
    "- We need to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of \n",
    "    - Greater robustness to individual observations\n",
    "    - Better classification of most of the training observations\n",
    "- It could be worthwile to misclassify a few training observations in order todo a better job in classifying the remaining observations. \n",
    "- The **support vector classfier (soft margin classifier)** allows some observations to be on the incorrect side of the margin, or even incorrect side of the hyperplane. \n",
    "- The margin is soft because it can be violated by some of the training observations. \n",
    "- Optimization of SVC : \n",
    "    - $maximize_{\\beta_0, \\beta_1, ..., \\beta_p, \\epsilon_1, ..., \\epsilon_n} M$ subject to $\\sum_{j=1}^p \\beta_j^2 = 1$,\n",
    "    - $y_i(\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_p x_{ip} \\geq M(1-\\epsilon_i)$\n",
    "    - $\\epsilon_i \\geq 0$, and $\\sum_{i=1}^n \\epsilon_i \\leq C$\n",
    "    - $C$ is a tuning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9ab9b",
   "metadata": {},
   "source": [
    "## 3.1 Margins and Slack Variables \n",
    "\n",
    "- $M$ is the width of the margin. \n",
    "- $\\epsilon_1, ..., \\epsilon_n$ are slack variables. \n",
    "    - If $\\epsilon_i = 0$, the $i$th obs. is on the correct side of the margin. \n",
    "    - If $\\epsilon_i > 0$, the $i$th obs. is on the wrong side of the margin. \n",
    "    - If $\\epsilon_i > 1$, the $i$th obs. is on the wrong side of the hyperplane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e1894",
   "metadata": {},
   "source": [
    "## 3.2 [Ex] Support Vector Classifier \n",
    "\n",
    "```R\n",
    "# Simple example (simulate data set)\n",
    "set.seed(1)\n",
    "x <- matrix(rnorm(20*2), ncol=2)\n",
    "y <- c(rep(-1, 10), rep(1, 10))\n",
    "x[y==1, ] <- x[y==1, ] + 1\n",
    "plot(x, col=(3-y), pch=19, xlab=\"X1\", ylab=\"X2\")\n",
    "```\n",
    "\n",
    "```R\n",
    "# Support vector classifier with cost=10\n",
    "library(e1071)\n",
    "# y must be in format (-1, 1) \n",
    "dat <- data.frame(x, y=as.factor(y))\n",
    "# Tuning parameter cost (inverse of C) \n",
    "svmfit <- svm(y~., data=dat, kernel=\"linear\", cost=10, scale=FALSE)\n",
    "plot(svmfit, dat)\n",
    "summary(svmfit)\n",
    "```\n",
    "\n",
    "<img src=\"Img/SVM2.png\" wdith=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd272b9",
   "metadata": {},
   "source": [
    "## 3.3 [Ex] Support Vector Classifier with different margins\n",
    "\n",
    "```R\n",
    "# SVM of tuning parameter (cost=0.1) \n",
    "svmfit <- svm(y~., data=dat, kernel=\"linear\", cost=0.1,\n",
    "scale=FALSE)\n",
    "svmfit$index\n",
    "beta <- drop(t(svmfit$coefs)%*%x[svmfit$index,])\n",
    "beta0 <- svmfit$rho\n",
    "\n",
    "# Visualize results\n",
    "plot(x, col=(3-y), pch=19, xlab=\"X1\", ylab=\"X2\")\n",
    "points(x[svmfit$index, ], pch=5, cex=2)\n",
    "abline(beta0 / beta[2], -beta[1] / beta[2])\n",
    "abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)\n",
    "abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)\n",
    "```\n",
    "\n",
    "<img src=\"Img/SVM3.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a02f1",
   "metadata": {},
   "source": [
    "## 3.4 [Ex] Support Vector Classifier with Hyper parameter tuning\n",
    "\n",
    "```R\n",
    "# Cross validation to find the optimal tuning parameter (cost) \n",
    "# Training using tune function \n",
    "set.seed(1)\n",
    "tune.out <- tune(svm, y~., data=dat, kernel=\"linear\",\n",
    "                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n",
    "summary(tune.out)\n",
    "bestmod <- tune.out$best.model\n",
    "summary(bestmod)\n",
    "\n",
    "# Generate test set \n",
    "set.seed(4321)\n",
    "xtest <- matrix(rnorm(20*2), ncol=2)\n",
    "ytest <- sample(c(-1,1), 20, rep=TRUE)\n",
    "xtest[ytest==1, ] <- xtest[ytest==1, ] + 1\n",
    "testdat <- data.frame(xtest, y=as.factor(ytest))\n",
    "\n",
    "# Calculate missclassification rate for optimal model \n",
    "# Compute misclassification rate for the optimal model\n",
    "ypred <- predict(bestmod, testdat)\n",
    "table(predict=ypred, truth=testdat$y)\n",
    "mean(ypred!=testdat$y)\n",
    "```\n",
    "\n",
    "- Missclassification error rate : 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b1253",
   "metadata": {},
   "source": [
    "## 3.5 [Ex] Simulation study of Support Vector Classifier\n",
    "\n",
    "```R\n",
    "# Prepare package \n",
    "library(mnormt)\n",
    "library(e1071)\n",
    "\n",
    "# Function for calculating Misclassification Rate of SVC\n",
    "SVC.MCR <- function(x.tran, x.test, y.tran, y.test,\n",
    "                    cost=c(0.01,0.1,1,10,100)) {\n",
    "  dat <- data.frame(x.tran, y=as.factor(y.tran))\n",
    "  testdat <- data.frame(x.test, y=as.factor(y.test))\n",
    "  MCR <- rep(0, length(cost)+1)\n",
    "  for (i in 1:length(cost)) {\n",
    "    svmfit <- svm(y~., data=dat, kernel=\"linear\",\n",
    "                  cost=cost[i])\n",
    "    MCR[i] <- mean(predict(svmfit, testdat)!=testdat$y)\n",
    "  }\n",
    "  tune.out <- tune(svm, y~., data=dat, kernel=\"linear\",\n",
    "                   ranges=list(cost=cost))\n",
    "  pred <- predict(tune.out$best.model, testdat)\n",
    "  MCR[length(cost)+1] <- mean(pred!=testdat$y)\n",
    "  MCR\n",
    "}\n",
    "\n",
    "# Simulation test for 100 replications\n",
    "set.seed(123)\n",
    "K <- 100\n",
    "RES <- matrix(NA, K, 6)\n",
    "colnames(RES) <- c(\"0.01\", \"0.1\" ,\"1\" , \"10\" ,\"100\", \"CV\")\n",
    "for (i in 1:K) {\n",
    "  x.A <- rmnorm(100, rep(0, 2), matrix(c(1,-0.5,-0.5,1),2))\n",
    "  x.B <- rmnorm(100, rep(1, 2), matrix(c(1,-0.5,-0.5,1),2))\n",
    "  x.tran <- rbind(x.A[1:50, ], x.B[1:50, ])\n",
    "  x.test <- rbind(x.A[-c(1:50), ], x.B[-c(1:50), ])\n",
    "  y.tran <- factor(rep(0:1, each=50))\n",
    "  y.test <- factor(rep(0:1, each=50))\n",
    "  RES[i,] <- SVC.MCR(x.tran, x.test, y.tran, y.test)\n",
    "}\n",
    "apply(RES, 2, summary)\n",
    "boxplot(RES, boxwex=0.5, col=2:7,\n",
    "        names=c(\"0.01\", \"0.1\", \"1\", \"10\", \"100\", \"CV\"),\n",
    "        main=\"\", ylab=\"Classification Error Rates\")\n",
    "```\n",
    "\n",
    "<img src=\"Img/SVM4.png\" width=\"400\" height=\"400\"> \n",
    "\n",
    "- The model with C=10/100 get best missclassification error rate of test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304cb00",
   "metadata": {},
   "source": [
    "# 4. Non-linear Support Vector Classifier \n",
    "\n",
    "- In practice, we can be faced with non-linear class boundaries. \n",
    "- We need to consider enlarging the feature space using functions of the predictors such as quadratic and cubic terms, in order to address this non-linearity. \n",
    "- In the case of the support vector classifier, we can also enlarge the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors. \n",
    "\n",
    "## 4.1 Feature Expension\n",
    "\n",
    "- Enlarge the space of features by including transformations : $X_1^2, X_1^3, ...$. \n",
    "- For example, $\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1^2 + \\beta_4 X_2^2 + \\beta_5 X_1 X_2 = 0$, This leads to non-linear decision boundaries in the original space.\n",
    "- The support vector classifier in the enlarged space solves the problem in the lower-dimensional space. \n",
    "\n",
    "## 4.2 Computational Issues for High-dimensional Polynomials\n",
    "\n",
    "- However, high-dimensional polynomials get wild rather fast. \n",
    "- There are many possible ways to enlarge the feature space, but computations would become unmanageable for a huge number of features. \n",
    "- There is a more elegant and controlled way to introduce nonlinearities in support vector classifiers, using kernels. \n",
    "\n",
    "## 4.3 Kernels and Support Vector Machines \n",
    "\n",
    "- The linear support vector classifier can be represented as : $f(x) = \\beta_0 + \\sum_{i \\in S} \\hat{\\alpha}_i K(x, x_i)$.\n",
    "- $S$ is the support set of indicies $i$ such that $\\alpha_i > 0$.\n",
    "- Parameters : \n",
    "    - $\\alpha_1, ..., \\alpha_n$\n",
    "    - $\\beta_0$\n",
    "    - $(n, 2)$ inner products \n",
    "- A generalization of the inner product of the form $K(x_i, x_{i'})$ is called a kernel.\n",
    "- A kernel is a function that quantifies the similarity of two observations. \n",
    "\n",
    "## 4.4 Examples of Kernels \n",
    "\n",
    "- Standard linear kernel : $K(x_i, x_{i'}) = \\sum_{j=1}^p x_{ij}x_{i'j}$\n",
    "- Polynomial kenrel of degree $d > 1$ : $K(x_i, x_{i'}) = (1 + \\sum_{j=1}^p x_{ij}x_{i'j})^d$\n",
    "    - We need to tune parameter d \n",
    "- Radial kernel : $K(x_i, x_{i'}) = exp(-\\gamma \\sum_{j=1}^p(x_{ij} - x_{i'j})^2)$\n",
    "\n",
    "![](Img/NLSVM1.png)\n",
    "\n",
    "- Left : SVM with a polynomial kernel of degree 3. \n",
    "- Right : SVM with a radial kernel. \n",
    "\n",
    "## 4.5 Computational Advantages \n",
    "\n",
    "- One advantage of using kernels over enlarging the feature space using function of original features is computational efficiency. \n",
    "- This is important because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11290048",
   "metadata": {},
   "source": [
    "# 5. Non-linear Support Vector Classifier \n",
    "\n",
    "## 5.1 [Ex] Non-linear SVMs with radial kernel \n",
    "\n",
    "```R\n",
    "# Simulate non-linear data set\n",
    "library(e1071)\n",
    "set.seed(1)\n",
    "x <- matrix(rnorm(200*2), ncol=2)\n",
    "x[1:100, ] <- x[1:100, ] + 2\n",
    "x[101:150, ] <- x[101:150, ] - 2\n",
    "y <- c(rep(-1, 150), rep(1, 50))\n",
    "dat <- data.frame(x, y=as.factor(y))\n",
    "plot(x, col=y+3, pch=19)\n",
    "\n",
    "# Training svm model with radial kernel with r=0.5, C=0.1\n",
    "fit <- svm(y~.,data=dat, kernel=\"radial\", gamma=0.5, cost=0.1)\n",
    "plot(fit, dat)\n",
    "summary(fit)\n",
    "\n",
    "# Training svm model with radial kernel with r=0.5, C=5\n",
    "fit <- svm(y~.,data=dat, kernel=\"radial\", gamma=0.5, cost=5)\n",
    "plot(fit, dat)\n",
    "summary(fit)\n",
    "\n",
    "# Visualize of test grid for radial kernel with r=0.5, C=1\n",
    "fit <- svm(y~.,data=dat, kernel=\"radial\", gamma=0.5, cost=1)\n",
    "px1 <- seq(round(min(x[,1]),1), round(max(x[,1]),1), 0.1)\n",
    "px2 <- seq(round(min(x[,2]),1), round(max(x[,2]),1), 0.1)\n",
    "xgrid <- expand.grid(X1=px1, X2=px2)\n",
    "ygrid <- as.numeric(predict(fit, xgrid))\n",
    "ygrid[ygrid==1] <- -1\n",
    "ygrid[ygrid==2] <- 1\n",
    "plot(xgrid, col=ygrid+3, pch = 20, cex = .2)\n",
    "points(x, col = y+3, pch = 19)\n",
    "pred <- predict(fit, xgrid, decision.values=TRUE)\n",
    "func <- attributes(pred)$decision\n",
    "contour(px1, px2, matrix(func, length(px1), length(px2)),\n",
    "level=0, col=\"purple\", lwd=2, lty=2, add=TRUE)\n",
    "```\n",
    "\n",
    "<img src=\"Img/NLSVM2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae7e4d",
   "metadata": {},
   "source": [
    "## 5.2 [Ex] Optimizing non-linear SVMs(radial kernel) using validation set \n",
    "\n",
    "```R\n",
    "# Calculate missclassification error of validation set \n",
    "# Separate training and test sets \n",
    "set.seed(1234)\n",
    "tran <- sample(200, 100)\n",
    "test <- setdiff(1:200, tran)\n",
    "\n",
    "# Training with hyperparameter tuning of gamma, C\n",
    "gamma <- c(0.5, 1, 5, 10)\n",
    "cost <- c(0.01, 1, 10, 100)\n",
    "R <- NULL\n",
    "for (i in 1:length(gamma)) {\n",
    "  for (j in 1:length(cost)) {\n",
    "    svmfit <- svm(y~., data=dat[tran, ], kernel=\"radial\",\n",
    "                  gamma=gamma[i] , cost=cost[j])\n",
    "    pred <- predict(svmfit, dat[test, ])\n",
    "    R0 <- c(gamma[i], cost[j], mean(pred!=dat[test, \"y\"]))\n",
    "    R <- rbind(R, R0)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Check results \n",
    "colnames(R) <- c(\"gamma\", \"cost\", \"error\")\n",
    "rownames(R) <- seq(dim(R)[1])\n",
    "R\n",
    "\n",
    "# Training with hyperparameter tuning of gamma, C using tune function \n",
    "set.seed(1)\n",
    "tune.out <- tune(svm, y~., data=dat[tran, ], kernel=\"radial\",\n",
    "                 ranges=list(gamma=gamma, cost=cost))\n",
    "summary(tune.out)\n",
    "tune.out$best.parameters\n",
    "\n",
    "# Calculate missclassification error rate of test sets\n",
    "pred <- predict(tune.out$best.model, dat[test,])\n",
    "table(pred=pred, true=dat[test, \"y\"])\n",
    "mean(pred!=dat[test, \"y\"])\n",
    "```\n",
    "\n",
    "- best parameters : gamma(0.5), cost(1) \n",
    "- Missclassification error rate : 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692f3d9",
   "metadata": {},
   "source": [
    "## 5.3 [Ex] Optimizing non-linear SVMs(polynomial) using validation set \n",
    "\n",
    "```R\n",
    "degree <- c(1, 2, 3, 4)\n",
    "R <- NULL\n",
    "\n",
    "for (i in 1:length(degree)) {\n",
    "  for (j in 1:length(cost)) {\n",
    "    svmfit <- svm(y~., data=dat[tran, ], kernel=\"polynomial\",\n",
    "                  degree=degree[i] , cost=cost[j])\n",
    "    pred <- predict(svmfit, dat[test, ])\n",
    "    R0 <- c(degree[i], cost[j], mean(pred!=dat[test, \"y\"]))\n",
    "    R <- rbind(R, R0)\n",
    "  }\n",
    "}\n",
    "colnames(R) <- c(\"degree\", \"cost\", \"error\")\n",
    "rownames(R) <- seq(dim(R)[1])\n",
    "R\n",
    "\n",
    "\n",
    "tune.out <- tune(svm, y~., data=dat[tran, ], kernel=\"polynomial\",\n",
    "                 ranges=list(degree=degree, cost=cost))\n",
    "summary(tune.out)\n",
    "tune.out$best.parameters\n",
    "\n",
    "pred <- predict(tune.out$best.model, dat[test,])\n",
    "table(pred=pred, true=dat[test, \"y\"])\n",
    "mean(pred!=dat[test, \"y\"])\n",
    "```\n",
    "\n",
    "- best parameters : degree(2), cost(10) \n",
    "- Missclassification error rate : 0.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d1833",
   "metadata": {},
   "source": [
    "## 5.4 [Ex] Optimizing non-linear SVMs(sigmoid) using validation set \n",
    "\n",
    "```R\n",
    "R <- NULL\n",
    "for (i in 1:length(gamma)) {\n",
    "  for (j in 1:length(cost)) {\n",
    "    svmfit <- svm(y~., data=dat[tran, ], kernel=\"sigmoid\",\n",
    "                  gamma=gamma[i] , cost=cost[j])\n",
    "    pred <- predict(svmfit, dat[test, ])\n",
    "    R0 <- c(gamma[i], cost[j], mean(pred!=dat[test, \"y\"]))\n",
    "    R <- rbind(R, R0)\n",
    "  }\n",
    "}\n",
    "\n",
    "colnames(R) <- c(\"gamma\", \"cost\", \"error\")\n",
    "rownames(R) <- seq(dim(R)[1])\n",
    "R\n",
    "\n",
    "tune.out <- tune(svm, y~., data=dat[tran, ], kernel=\"sigmoid\",\n",
    "                        ranges=list(gamma=gamma, cost=cost))\n",
    "summary(tune.out)\n",
    "tune.out$best.parameters\n",
    "\n",
    "pred <- predict(tune.out$best.model, dat[test,])\n",
    "table(pred=pred, true=dat[test, \"y\"])\n",
    "mean(pred!=dat[test, \"y\"])\n",
    "```\n",
    "\n",
    "- best parameters : gamma(0.5), cost(0.01) \n",
    "- Missclassification error rate : 0.29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47331d84",
   "metadata": {},
   "source": [
    "## 5.5 [Ex] Simulation study using different kernels of 20 replications \n",
    "\n",
    "```R\n",
    "# Set reps and RES matrix \n",
    "set.seed(123)\n",
    "N <- 20\n",
    "RES <- matrix(0, N, 3)\n",
    "colnames(RES) <- c(\"radial\", \"poly\", \"sigmoid\")\n",
    "\n",
    "# Training model with calculate missclassification error rate \n",
    "for (i in 1:N) {\n",
    "  tran <- sample(200, 100)\n",
    "  test <- setdiff(1:200, tran)\n",
    "  tune1 <- tune(svm, y~., data=dat[tran, ], kernel=\"radial\",\n",
    "                ranges=list(gamma=gamma, cost=cost))\n",
    "  pred1 <- predict(tune1$best.model, dat[test,])\n",
    "  RES[i, 1] <- mean(pred1!=dat[test, \"y\"])\n",
    "  tune2 <- tune(svm, y~., data=dat[tran, ], kernel=\"polynomial\",\n",
    "                ranges=list(degree=degree, cost=cost))\n",
    "  pred2 <- predict(tune2$best.model, dat[test,])\n",
    "  RES[i, 2] <- mean(pred2!=dat[test, \"y\"])\n",
    "  tune3 <- tune(svm, y~., data=dat[tran, ], kernel=\"sigmoid\",\n",
    "                ranges=list(gamma=gamma, cost=cost))\n",
    "  pred3 <- predict(tune3$best.model, dat[test,])\n",
    "  RES[i, 3] <- mean(pred3!=dat[test, \"y\"])\n",
    "}\n",
    "# Check statistical reports \n",
    "apply(RES, 2, summary)\n",
    "```\n",
    "\n",
    "|Statistics|radial|poly|sigmoid|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|Min|0.07|0.14|0.17|\n",
    "|1st|0.1|0.1675|0.2375|\n",
    "|Median|0.12|0.1950|0.26|\n",
    "|Mean|0.123|0.209|0.2705|\n",
    "|3rd|0.1425|0.235|0.29|\n",
    "|Max|0.18|0.31|0.43|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dcd87",
   "metadata": {},
   "source": [
    "# 6. Non-linear Support Vector Classifier on Heart.csv\n",
    "\n",
    "### Step 1 : Prepare Heart Dataset\n",
    "\n",
    "```R\n",
    "# Prepare Heart dataset \n",
    "url.ht <- \"https://www.statlearning.com/s/Heart.csv\"\n",
    "Heart <- read.csv(url.ht, h=T)\n",
    "summary(Heart)\n",
    "Heart <- Heart[, colnames(Heart)!=\"X\"]\n",
    "Heart[,\"Sex\"] <- factor(Heart[,\"Sex\"], 0:1, c(\"female\", \"male\"))\n",
    "Heart[,\"Fbs\"] <- factor(Heart[,\"Fbs\"], 0:1, c(\"false\", \"true\"))\n",
    "Heart[,\"ExAng\"] <- factor(Heart[,\"ExAng\"], 0:1, c(\"no\", \"yes\"))\n",
    "Heart[,\"ChestPain\"] <- as.factor(Heart[,\"ChestPain\"])\n",
    "Heart[,\"Thal\"] <- as.factor(Heart[,\"Thal\"])\n",
    "Heart[,\"AHD\"] <- as.factor(Heart[,\"AHD\"])\n",
    "summary(Heart)\n",
    "dim(Heart)\n",
    "sum(is.na(Heart))\n",
    "Heart <- na.omit(Heart)\n",
    "dim(Heart)\n",
    "summary(Heart)\n",
    "```\n",
    "\n",
    "### Step 2 : Separate training and test sets \n",
    "\n",
    "```R\n",
    "# Separate training and test sets \n",
    "set.seed(123)\n",
    "train <- sample(1:nrow(Heart), nrow(Heart)/2)\n",
    "test <- setdiff(1:nrow(Heart), train)\n",
    "```\n",
    "\n",
    "### Step 3 : Training using SVMs \n",
    "\n",
    "```R\n",
    "# SVM with a linear kernel\n",
    "tune.out <- tune(svm, AHD~., data=Heart[train, ],\n",
    "                 kernel=\"linear\", ranges=list(\n",
    "                   cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n",
    "heart.pred <- predict(tune.out$best.model, Heart[test,])\n",
    "table(heart.pred, Heart$AHD[test])\n",
    "mean(heart.pred!=Heart$AHD[test])\n",
    "\n",
    "# SVM with a radial kernel\n",
    "tune.out <- tune(svm, AHD~., data=Heart[train, ],\n",
    "                 kernel=\"radial\", ranges=list(\n",
    "                   cost=c(0.1,1,10,100), gamma=c(0.5,1,2,3)))\n",
    "heart.pred <- predict(tune.out$best.model, Heart[test,])\n",
    "table(heart.pred, Heart$AHD[test])\n",
    "mean(heart.pred!=Heart$AHD[test])\n",
    "\n",
    "# SVM with a polynomial kernel\n",
    "tune.out <- tune(svm, AHD~.,data=Heart[train, ],\n",
    "                 kernel=\"polynomial\", ranges=list(\n",
    "                   cost=c(0.1,1,10,100), degree=c(1,2,3)))\n",
    "heart.pred <- predict(tune.out$best.model, Heart[test,])\n",
    "table(heart.pred, Heart$AHD[test])\n",
    "mean(heart.pred!=Heart$AHD[test])\n",
    "\n",
    "# SVM with a sigmoid kernel\n",
    "tune.out <- tune(svm, AHD~.,data=Heart[train, ],\n",
    "                 kernel=\"sigmoid\", ranges=list(\n",
    "                   cost=c(0.1,1,10,100), gamma=c(0.5,1,2,3)))\n",
    "heart.pred <- predict(tune.out$best.model, Heart[test,])\n",
    "table(heart.pred, Heart$AHD[test])\n",
    "mean(heart.pred!=Heart$AHD[test])\n",
    "```\n",
    "\n",
    "|kernels|Missclassification error rate|\n",
    "|:---:|:---:|\n",
    "|linear|0.2214765|\n",
    "|radial|0.2080537|\n",
    "|polynomials|0.2080537|\n",
    "|sigmoid|0.2147651|\n",
    "\n",
    "### Step 4 : Simulation Study using different kernels of 20 replications \n",
    "\n",
    "```R\n",
    "set.seed(123)\n",
    "N <- 20\n",
    "Err <- matrix(0, N, 5)\n",
    "\n",
    "for (i in 1:N) {\n",
    "  train <- sample(1:nrow(Heart), floor(nrow(Heart)*2/3))\n",
    "  test <- setdiff(1:nrow(Heart), train)\n",
    "  g1 <- randomForest(x=Heart[train,-14], y=Heart[train,14],\n",
    "                     xtest=Heart[test,-14], ytest=Heart[test,14], mtry=4)\n",
    "  Err[i,1] <- g1$test$err.rate[500,1]\n",
    "  g2 <- tune(svm, AHD~., data=Heart[train, ], kernel=\"linear\",\n",
    "             ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n",
    "  p2 <- predict(g2$best.model, Heart[test,])\n",
    "  Err[i,2] <- mean(p2!=Heart$AHD[test])\n",
    "  g3 <- tune(svm, AHD~., data=Heart[train, ], kernel=\"radial\",\n",
    "             ranges=list(cost=c(0.1,1,10,100), gamma=c(0.5,1,2,3)))\n",
    "  p3 <- predict(g3$best.model, Heart[test,])\n",
    "  Err[i,3] <- mean(p3!=Heart$AHD[test])\n",
    "  \n",
    "  g4 <- tune(svm, AHD~.,data=Heart[train, ],kernel=\"polynomial\",\n",
    "             ranges=list(cost=c(0.1,1,10,100), degree=c(1,2,3)))\n",
    "  p4 <- predict(g4$best.model, Heart[test,])\n",
    "  Err[i,4] <- mean(p4!=Heart$AHD[test])\n",
    "  g5 <- tune(svm, AHD~.,data=Heart[train, ],kernel=\"sigmoid\",\n",
    "             ranges=list(cost=c(0.1,1,10,100), gamma=c(0.5,1,2,3)))\n",
    "  p5 <- predict(g5$best.model, Heart[test,])\n",
    "  Err[i,5] <- mean(p5!=Heart$AHD[test])\n",
    "}\n",
    "\n",
    "# Visualize results \n",
    "labels <- c(\"RF\",\"SVM.linear\",\"SVM.radial\",\"SVM.poly\",\"SVM.sig\")\n",
    "boxplot(Err, boxwex=0.5, main=\"Random Forest and SVM\", col=2:6,\n",
    "        names=labels, ylab=\"Classification Error Rates\",\n",
    "        ylim=c(0,0.4))\n",
    "colnames(Err) <- labels\n",
    "apply(Err, 2, summary)\n",
    "```\n",
    "\n",
    "<img src=\"Img/NLSVM3.png\" width=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "428.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
