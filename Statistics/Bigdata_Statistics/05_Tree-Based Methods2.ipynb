{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6563a853",
   "metadata": {},
   "source": [
    "# 1. Ensemble \n",
    "\n",
    "- An **ensemble** method is an approach that combines many simple building ensemble block models in order to obtain a single and potentially very powerful model. \n",
    "- These simple building block models are sometimes known as weak learners. \n",
    "- Methods \n",
    "    - Bagging \n",
    "    - Random forest \n",
    "    - Boosting \n",
    "    - Bayesian additive regression trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ac8e",
   "metadata": {},
   "source": [
    "# 2. Bagging \n",
    "\n",
    "## 2.1 Bootstrap methods\n",
    "\n",
    "1. Referring $(X_1, ..., X_n)$ as population, sample n with replacement \n",
    "2. Iterating n times : making n Bootstrap sets \n",
    "    1. Calculating statistics from sampled sets $(\\hat{X_1}, ..., \\hat{X_n})$\n",
    "    2. Calculating aggregated statistics from Bootstrap sets \n",
    "\n",
    "```R\n",
    "# Population of training set \n",
    "seq(20) \n",
    "\n",
    "# Boostrap set \n",
    "sort(sample(seq(20), 20))\n",
    "```\n",
    "\n",
    "## 2.2 Bagging Tree methods \n",
    "\n",
    "- **Bootstrap aggregation**, or **bagging**, is a general-purpose procedure for reducing the variance of a statistical learning method \n",
    "- Repeat calculating statistical with every sampled sets : $\\frac{1}{B}\\sum_{i=1}^{B} \\bar{X_i}$ \n",
    "- Taking repeated samples from the training set. \n",
    "- Generate B different bootstrapped training data sets : $\\hat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^b(x)$\n",
    "- For classification trees : for each test observation, we record the class predicted by each of the B trees, and take a majority vote: the overall prediction is the most commonly occuring class among the B predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fda64",
   "metadata": {},
   "source": [
    "## 2.3 [Ex] Bagging Classification Tree\n",
    "\n",
    "```R\n",
    "# Separate training and test sets\n",
    "set.seed(123)\n",
    "train <- sample(1:nrow(Heart), nrow(Heart)/2)\n",
    "test <- setdiff(1:nrow(Heart), train)\n",
    "\n",
    "# Classification error rate for single tree \n",
    "heart.tran <- tree(AHD ~., subset=train, Heart)\n",
    "heart.pred <- predict(heart.tran, Heart[test, ], type=\"class\")\n",
    "tree.err <- mean(Heart$AHD[test]!=heart.pred)\n",
    "tree.err\n",
    "\n",
    "# Bagging \n",
    "set.seed(12345)\n",
    "B <- 500\n",
    "n <- nrow(Heart)\n",
    "Vote <- rep(0, length(test))\n",
    "bag.err <- NULL \n",
    "\n",
    "for (i in 1:B) {\n",
    "    # Bootstrap training set \n",
    "    index <- sample(train, replace=TRUE)\n",
    "    heart.tran <- tree(AHD ~., Heart[index,])\n",
    "    heart.pred <- predict(heart.tran, Heart[test, ], type=\"class\")\n",
    "    Vote[heart.pred==\"Yes\"] <- Vote[heart.pred==\"Yes\"] + 1\n",
    "    preds <- rep(\"Yes\", length(test))\n",
    "    # Decide as \"No\" when the number of voted case is lower than i/2 \n",
    "    # Apply majority rules\n",
    "    preds[Vote < i/2] <- \"No\"\n",
    "    bag.err[i] <- mean(Heart$AHD[test]!=preds)\n",
    "}\n",
    "\n",
    "# Visualize bagging decision tree \n",
    "plot(bag.err, type=\"l\", xlab=\"Number of Trees\", col=1, ylab=\"Classification Error Rate\")\n",
    "abline(h=tree.err, lty=2, col=2)\n",
    "legend(\"topright\", c(\"Single tree\", \"Bagging\"), col=c(2,1), lty=c(2,1))\n",
    "```\n",
    "\n",
    "![](Img/Ensemble1.png)\n",
    "\n",
    "- Missclassification rate converges to 0.23xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14aac7",
   "metadata": {},
   "source": [
    "# 3. Out-of-Bag Error Estimation\n",
    "\n",
    "- The key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. \n",
    "- One can show that on average, each bagged tree makes use of around two-thirds of the observations \n",
    "- The remaining one-third of the observations not used to fit a given bagged tree are referred to as the **out-of-bag(OOB)** observations. \n",
    "- We can predict the response for the $i$th observations using each of the trees in which that observation was **OOB**. This will yield around $\\frac{B}{3}$ predictions for the $i$th observation on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa958c4d",
   "metadata": {},
   "source": [
    "## 3.1 [Ex] Average of missclassification rate of single tree \n",
    "\n",
    "```R\n",
    "# Average of missclassification rate of single tree\n",
    "# Over 50 replications\n",
    "set.seed(12345)\n",
    "K <- 50\n",
    "Err <- NULL \n",
    "\n",
    "for (i in 1:K) {\n",
    "  train <- sample(1:nrow(Heart), nrow(Heart)*2/3) \n",
    "  test <- setdiff(1:nrow(Heart), train) \n",
    "  heart.tran <- tree(AHD ~., subset=train, Heart)\n",
    "  heart.pred <- predict(heart.tran, Heart[test, ], type=\"class\") \n",
    "  Err[i] <- mean(Heart$AHD[test]!=heart.pred)\n",
    "}\n",
    "summary(Err)\n",
    "Tree.Err <- mean(Err)\n",
    "```\n",
    "\n",
    "|Min|1st|Median|Mean|3rd|Max|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|0.1616|0.2121|0.2424|0.2473|0.2727|0.3333| \n",
    "\n",
    "- Over 50 replacations, the mean of misscalssification rates is 0.2424."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52fc46",
   "metadata": {},
   "source": [
    "## 3.2 [Ex] Out-of-Bagging missclassification rate \n",
    "\n",
    "```R\n",
    "# OOB \n",
    "set.seed(1234)\n",
    "Valid <- Vote <- Mis <- rep(0, nrow(Heart)) \n",
    "OOB.err <- NULL\n",
    "\n",
    "for (i in 1:B) {\n",
    "  # Bootstrapping from Heart index \n",
    "  index <- sample(1:nrow(Heart), replace=TRUE)\n",
    "  # Extract test index from boostrapped index \n",
    "  test <- setdiff(1:nrow(Heart), unique(index))\n",
    "  Valid[test] <- Valid[test] + 1\n",
    "  # Train model with bootstrapped training sample \n",
    "  heart.tran <- tree(AHD ~., Heart[index,])\n",
    "  # Make predictions of test sets \n",
    "  heart.pred <- predict(heart.tran, Heart[test,], type=\"class\")\n",
    "  Vote[test] <- Vote[test] + (heart.pred==\"Yes\")\n",
    "  # Vote for test sets \n",
    "  preds <- rep(\"Yes\", length(test))\n",
    "  preds[Vote[test]/Valid[test] < 0.5] <- \"No\"\n",
    "  # Find index of misscalssified case \n",
    "  wh <- which(Heart$AHD[test]!=preds)\n",
    "  Mis[test[wh]] <- -1\n",
    "  Mis[test[-wh]] <- 1\n",
    "  OOB.err[i] <- sum(Mis==-1)/sum(Mis!=0)\n",
    "}\n",
    "\n",
    "# View statistical reports of error \n",
    "summary(OOB.err)\n",
    "summary(OOB.err[-c(1:100)])\n",
    "\n",
    "# Visualize results \n",
    "plot(OOB.err, type=\"l\", xlab=\"Number of Trees\", col=1,\n",
    "     ylab=\"Classification Error Rate\", ylim=c(0.1,0.4))\n",
    "abline(h=Tree.Err, lty=2, col=2)\n",
    "legend(\"topright\", c(\"Single tree\", \"OOB\"), col=c(2,1), lty=c(2,1))\n",
    "```\n",
    "\n",
    "<img src=\"Img/OOB1.png\" width=\"300\" height=\"150\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d9d70",
   "metadata": {},
   "source": [
    "# 4. Random Forest\n",
    "\n",
    "- **Random forests** provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. This reduces the variance when we average the trees. \n",
    "- When building these decision trees, each time a split in a trees is considered, <span style=\"color:red\"> a random selection of m predictors is chosen as split candidates from the full set of p predictors</span>. \n",
    "- A fresh selection of m predictors is taken at each split, and typically we choose $m=\\sqrt{p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54da467",
   "metadata": {},
   "source": [
    "## 4.1 [Ex] RandomForest with m predictors \n",
    "\n",
    "```R\n",
    "# Train-test split\n",
    "set.seed(123)\n",
    "train <- sample(1:nrow(Heart), nrow(Heart)/2) \n",
    "test <- setdiff(1:nrow(Heart), train) \n",
    "\n",
    "# Bagging: m=13, Random forest: m=1, 4, 6\n",
    "B <- 500 \n",
    "n <- nrow(Heart)\n",
    "m <- c(1, 4, 6, 13) \n",
    "Err <- matrix(0, B, length(m)) \n",
    "Vote <- matrix(0, length(test), length(m)) \n",
    "\n",
    "for (i in 1:B) { \n",
    "    index <- sample(train, replace=TRUE) \n",
    "    for (k in 1:length(m)) { \n",
    "        s <- c(sort(sample(1:13, m[k])), 14) \n",
    "        tr <- tree(AHD ~., data=Heart[index, s]) \n",
    "        pr <- predict(tr, Heart[test, ], type=\"class\") \n",
    "        Vote[pr==\"Yes\", k] <- Vote[pr==\"Yes\", k] + 1\n",
    "        PR <- rep(\"Yes\", length(test)) \n",
    "        PR[Vote[,k] < i/2] <- \"No\" \n",
    "        Err[i, k] <- mean(Heart$AHD[test]!=PR) \n",
    "    } \n",
    "} \n",
    "\n",
    "# Visualize Result \n",
    "labels <- c(\"m = 1\", \"m = 4\", \"m = 6\", \"m = 13\")\n",
    "matplot(Err, type=\"l\", xlab=\"Number of Trees\", lty=1,\n",
    "col=c(1,2:4), ylab=\"Classification Error Rate\")\n",
    "legend(\"topright\", legend=labels, col=c(1,2:4), lty=1)\n",
    "\n",
    "# Statistical reports \n",
    "colnames(Err) <- labels\n",
    "apply(Err, 2, summary)\n",
    "```\n",
    "\n",
    "![](Img/RandomForest1.png)\n",
    "\n",
    "- m = 1 : 0.2408725 \n",
    "- m = 4 : 0.2242282 \n",
    "- m = 6 : 0.2242550\n",
    "- m = 13 : 0.2466040\n",
    "- The missclassification rate was the lowest when m was 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8c350d",
   "metadata": {},
   "source": [
    "## 4.2 [Ex] Random Forest using randomForest packages\n",
    "\n",
    "```R\n",
    "library(randomForest)\n",
    "\n",
    "# Bagging(m=13) \n",
    "bag.heart <- randomForest(x=Heart[train, -14], y=Heart[train,14], \n",
    "                          xtest=Heart[test, -14], ytest=Heart[test,14], \n",
    "                          mtry=13, importance=TRUE) \n",
    "bag.heart\n",
    "bag.conf <- bag.heart$test$confusion[1:2,1:2] \n",
    "\n",
    "# Missclassification Error of m = 13\n",
    "1 - sum(diag(bag.conf))/sum(bag.conf)\n",
    "\n",
    "# Bagging(m=1) \n",
    "rf1 <- randomForest(x=Heart[train,-14], y=Heart[train,14], \n",
    "                    xtest=Heart[test,-14], ytest=Heart[test,14], \n",
    "                    mtry=1, importance=TRUE) \n",
    "rf1.conf <- rf1$test$confusion[1:2, 1:2]\n",
    "1- sum(diag(rf1.conf))/sum(rf1.conf)\n",
    "\n",
    "## Random forest with m=4\n",
    "rf2 <- randomForest(x=Heart[train,-14], y=Heart[train,14],\n",
    "                    xtest=Heart[test,-14], ytest=Heart[test,14],\n",
    "                    mtry=4, importance=TRUE)\n",
    "rf2.conf <- rf2$test$confusion[1:2,1:2]\n",
    "1- sum(diag(rf2.conf))/sum(rf2.conf)\n",
    "\n",
    "## Random forest with m=6\n",
    "rf3 <- randomForest(x=Heart[train,-14], y=Heart[train,14],\n",
    "                    xtest=Heart[test,-14], ytest=Heart[test,14],\n",
    "                    mtry=6, importance=TRUE)\n",
    "rf3.conf <- rf3$test$confusion[1:2,1:2]\n",
    "1- sum(diag(rf3.conf))/sum(rf3.conf)\n",
    "```\n",
    "\n",
    "- Missclassification Error of m = 13 : 0.2684564\n",
    "- Missclassification Error of m = 1 : 0.2147651\n",
    "- Missclassification Error of m = 4 : 0.2416107\n",
    "- Missclassification Error of m = 6 : 0.2348993 \n",
    "\n",
    "**randomForest(x, y=NULL, xtest=NULL, ytest=NULL, ntree=500, mtry=n, replace=TRUE)** \n",
    "\n",
    "- x, y : x, y can be applied separately, usually using formula a lot \n",
    "- xtest, ytest : If we apply the test dataset together, perform the test at the same time\n",
    "- ntree : The number of the tree \n",
    "- mtry : The number of descriptive variable candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af295ce7",
   "metadata": {},
   "source": [
    "## 4.3 [Ex] Random Forest using mtry grid search\n",
    "\n",
    "```R\n",
    "# Set Grids \n",
    "set.seed(1111)\n",
    "N <- 50\n",
    "CER <- matrix(0, N, 13)\n",
    "\n",
    "# Training models : 50 replications \n",
    "for (i in 1:N) {\n",
    "    train <- sample(1:nrow(Heart), floor(nrow(Heart)*2/3))\n",
    "    test <- setdiff(1:nrow(Heart), train)\n",
    "    for (k in 1:13) {\n",
    "        # Apply random forest \n",
    "        rf <- randomForest(x=Heart[train, -14], y=Heart[train, 14], \n",
    "                           xtest=Heart[test, -14], ytest=Heart[test, 14], mtry=k)\n",
    "        rfc <- rf$test$confusion[1:2,1:2]\n",
    "        # Calculate missclassification rate \n",
    "        CER[i, k] <- 1-sum(diag(rfc))/sum(rfc)\n",
    "    }\n",
    "}\n",
    "apply(CER, 2, mean)\n",
    "\n",
    "# Visualize results \n",
    "boxplot(CER, boxwex=0.5, main=\"Random Forest with m = 1 to 13\",\n",
    "        ylab=\"Classification Error Rates\", col=\"orange\", ylim=c(0, 0.4))\n",
    "```\n",
    "\n",
    "<img src=\"Img/RandomForest2.png\" height=400 width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc5853",
   "metadata": {},
   "source": [
    "## 4.3 [Ex] Variable Importance Reports \n",
    "\n",
    "```R\n",
    "# Scatter plot of feature importance \n",
    "varImpPlot(rf1)\n",
    "varImpPlot(rf2)\n",
    "varImpPlot(rf3)\n",
    "\n",
    "# Horizontal bar plot of feature importance \n",
    "(imp1 <- importance(rf1))\n",
    "(imp2 <- importance(rf2))\n",
    "(imp3 <- importance(rf3))\n",
    "\n",
    "par(mfrow=c(1,3))\n",
    "# Based on MeanDecreaseAccuracy\n",
    "barplot(sort(imp1[,3]), main=\"RF (m=1)\", horiz=TRUE, col=2)\n",
    "barplot(sort(imp2[,3]), main=\"RF (m=4)\", horiz=TRUE, col=2)\n",
    "barplot(sort(imp3[,3]), main=\"RF (m=6)\", horiz=TRUE, col=2)\n",
    "\n",
    "# Based on MeanDecreaseGini\n",
    "barplot(sort(imp1[,4]), main=\"RF (m=1)\", horiz=TRUE, col=2)\n",
    "barplot(sort(imp2[,4]), main=\"RF (m=4)\", horiz=TRUE, col=2)\n",
    "barplot(sort(imp3[,4]), main=\"RF (m=6)\", horiz=TRUE, col=2)\n",
    "```\n",
    "\n",
    "- MeanDecreaseAccuracy : Shows feature importance when splitting nodes based on Accuracy\n",
    "- MeanDecreaseGini : Shows feature importance when splitting nodes based on Gini index \n",
    "\n",
    "<img src=\"Img/RandomForest3.png\">\n",
    "<img src=\"Img/RandomForest4.png\">\n",
    "<img src=\"Img/RandomForest5.png\">\n",
    "<img src=\"Img/RandomForest6.png\">\n",
    "<img src=\"Img/RandomForest7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9dce4",
   "metadata": {},
   "source": [
    "## 4.5 [Ex] Assignments \n",
    "\n",
    "The Caravan insurance dataset consists of 85 quantitative variables and 1 factor with 2 levels. The factor is a response variable Purchase. Randomly separate the training. validation and test sets such that \n",
    "\n",
    "```\n",
    "set.seed(1111)\n",
    "data(Caravan)\n",
    "train <- sample(1:nrow(Caravan), nrow(Caravan)/2)\n",
    "others <- setdiff(1:nrow(Caravan), train)\n",
    "validation <- sample(others, length(others)/2)\n",
    "test <- setdiff(others, validation) \n",
    "```\n",
    "\n",
    "Fit the random forest model using the training set, where the number of predictors begins from 1 to 9. First, find the optimal number of predictors using the validation set which minimizes the classification error rate(CER). Then, compute the CER of the test set, using the random forest with the optimal number of predictors. \n",
    "\n",
    "```R\n",
    "library(randomForest)\n",
    "library(ISLR)\n",
    "\n",
    "set.seed(1111)\n",
    "data(Caravan)\n",
    "train <- sample(1:nrow(Caravan), nrow(Caravan)/2)\n",
    "others <- setdiff(1:nrow(Caravan), train)\n",
    "validation <- sample(others, length(others)/2)\n",
    "test <- setdiff(others, validation) \n",
    "\n",
    "CER <- NULL\n",
    "for (i in 1:9) { \n",
    "  rf <- randomForest(x=Caravan[train, -86], y=Caravan[train, 86],\n",
    "                     xtest=Caravan[validation, -86], ytest=Caravan[validation, 86], mtry=i)  \n",
    "  rfc <- rf$test$confusion[1:2,1:2]\n",
    "  CER[i] <- 1-sum(diag(rfc))/sum(rfc)\n",
    "}\n",
    "wm <- which.min(CER)\n",
    "\n",
    "# Calculate missclassification of test sets \n",
    "rf.test <- randomForest(x=Caravan[train, -86], y=Caravan[train, 86], \n",
    "                   xtest=Caravan[test, -86], ytest=Caravan[test, 86], mtry=wm)\n",
    "rfc.test <- rf.test$test$confusion[1:2,1:2]\n",
    "CER.test <- 1-sum(diag(rfc.test))/sum(rfc.test)\n",
    "CER.test\n",
    "```\n",
    "\n",
    "- wm : 3\n",
    "- CER.test : 0.06593407"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51477b25",
   "metadata": {},
   "source": [
    "# 5. Boosting \n",
    "\n",
    "- **Boosting** is a general approach that can be applied to many statistical learning methods for regression or classification. \n",
    "- Boosting grow trees sequentially. \n",
    "- The boosting approach learns slowly. \n",
    "- Given the current model, we fit a decision tree to the residuals from the model. We then add this new decision tree into the fitted function in order to update the residuals. \n",
    "- Each of these models can be rather small, with just a few terminal nodes, determined by the parameter $d$ in the algorithm. \n",
    "- The kinds of boosting algorithms\n",
    "    - XGBoost \n",
    "    - Adaboost \n",
    "    \n",
    "    \n",
    "## 5.1 Boosting algorithm \n",
    "\n",
    "1. Set $\\hat{f}(x) = 0$ and $r_i = y_i$ for all $i$ in the training set. \n",
    "2. For $b = 1, 2, ..., B$, repeat : \n",
    "    1. Fit a tree $\\hat{f}^b$ with $d$ splits to the training data $(X, r)$. \n",
    "    2. Update $\\hat{f}$ by adding in a shrunken version of the new tree : $\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda\\hat{f}^b(x)$\n",
    "    3. Update the residuals, $r_i \\leftarrow r_i - \\lambda\\hat{f}^b(x_i)$.\n",
    "3. Output the boosted model : $\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f99351",
   "metadata": {},
   "source": [
    "## 5.2 Tuning parameters for boosting \n",
    "\n",
    "- The number of trees $B$ : Boosting can be overfit if $B$ is too large, although this overfitting tends to occur slowly if at all.\n",
    "- The shrinkage parameter $\\lambda$ : Controls the rate at which boosting learns. Typical values are 0.01 or 0.001.\n",
    "- The number of splits $d$ in each tree : Controls the complexity of the boosted ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79c2aa",
   "metadata": {},
   "source": [
    "## 5.3 [Ex] Boosting Decision Tree : Classification \n",
    "\n",
    "```R\n",
    "# Prerequirisite : Importing library, dataset, preprocessing \n",
    "library(gbm)\n",
    "\n",
    "# Train-Test split \n",
    "set.seed(123)\n",
    "train <- sample(1:nrow(Heart), nrow(Heart)/2)\n",
    "test <- setdiff(1:nrow(Heart), train)\n",
    "\n",
    "# Create (0,1) response\n",
    "Heart0 <- Heart\n",
    "Heart0[,\"AHD\"] <- as.numeric(Heart$AHD)-1\n",
    "\n",
    "# boosting (d=1)\n",
    "boost.d1 <- gbm(AHD~., data=Heart0[train, ], n.trees=1000,\n",
    "distribution=\"bernoulli\", interaction.depth=1)\n",
    "\n",
    "# Results of feature selection \n",
    "summary(boost.d1)\n",
    "\n",
    "# Calcualte missclassification error \n",
    "# We need to set value of keyword n.trees for boosting trees \n",
    "yhat.d1 <- predict(boost.d1, newdata=Heart0[test, ], type=\"response\", n.trees=1000)\n",
    "phat.d1 <- rep(0, length(yhat.d1))\n",
    "phat.d1[yhat.d1 > 0.5] <- 1\n",
    "mean(phat.d1!=Heart0[test, \"AHD\"])\n",
    "```\n",
    "\n",
    "```R\n",
    "# boosting (d=2)\n",
    "# Training model with max_depth = 2\n",
    "boost.d2 <- gbm(AHD~., data=Heart0[train, ], n.trees=1000,\n",
    "distribution=\"bernoulli\", interaction.depth=2)\n",
    "\n",
    "# Make predictions \n",
    "yhat.d2 <- predict(boost.d2, newdata=Heart0[test, ], type=\"response\", n.trees=1000)\n",
    "phat.d2 <- rep(0, length(yhat.d2))\n",
    "phat.d2[yhat.d2 > 0.5] <- 1\n",
    "mean(phat.d2!=Heart0[test, \"AHD\"])\n",
    "```\n",
    "\n",
    "```R\n",
    "# boosting (d=3)\n",
    "# Training model with max_depth = 3\n",
    "boost.d3 <- gbm(AHD~., data=Heart0[train, ], n.trees=1000,\n",
    "distribution=\"bernoulli\", interaction.depth=3)\n",
    "\n",
    "# Make predictions\n",
    "yhat.d3 <- predict(boost.d3, newdata=Heart0[test, ], type=\"response\", n.trees=1000)\n",
    "phat.d3 <- rep(0, length(yhat.d3))\n",
    "phat.d3[yhat.d3 > 0.5] <- 1\n",
    "mean(phat.d3!=Heart0[test, \"AHD\"])\n",
    "```\n",
    "\n",
    "```R\n",
    "# boosting (d=4)\n",
    "# Training model with max_depth = 4\n",
    "boost.d4 <- gbm(AHD~., data=Heart0[train, ], n.trees=1000,\n",
    "distribution=\"bernoulli\", interaction.depth=4)\n",
    "\n",
    "# Make predictions \n",
    "yhat.d4 <- predict(boost.d4, newdata=Heart0[test, ], type=\"response\", n.trees=1000)\n",
    "phat.d4 <- rep(0, length(yhat.d4))\n",
    "phat.d4[yhat.d4 > 0.5] <- 1\n",
    "mean(phat.d4!=Heart0[test, \"AHD\"])\n",
    "```\n",
    "\n",
    "<img src=\"Img/Boosting1.png\">\n",
    "\n",
    "|gbm(iteracion.depth=1)|gbm(iteraction.depth=2)|gbm(iteraction.depth=3)|gbm(interaction.depth=4)|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|0.2013423|0.2013423|0.2281879|0.1879105|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92ed66",
   "metadata": {},
   "source": [
    "## 5.4 [Ex] Hyperparameter tuning : n.trees, interaction.depth\n",
    "\n",
    "```R\n",
    "# Simulation: Boosting with d=1, 2, 3 and 4\n",
    "# The number of trees: 1 to 3000\n",
    "\n",
    "# Set grids \n",
    "set.seed(1111)\n",
    "Err <- matrix(0, 3000, 4)\n",
    "\n",
    "# Training models with n.trees, interaction.depth : 1 to 4\n",
    "for (k in 1:4) {\n",
    "    boost <- gbm(AHD~., data=Heart0[train, ], n.trees=3000,\n",
    "distribution=\"bernoulli\", interaction.depth=k)\n",
    "    for (i in 1:3000) {\n",
    "        # Make predictions of n.trees : 1 to 3000 \n",
    "        yhat <- predict(boost, newdata=Heart0[test, ],\n",
    "        type=\"response\", n.trees=i)\n",
    "        phat <- rep(0, length(yhat))\n",
    "        phat[yhat > 0.5] <- 1\n",
    "        Err[i,k] <- mean(phat!=Heart0[test, \"AHD\"])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Visualize results \n",
    "labels <- c(\"d = 1\", \"d = 2\", \"d = 3\", \"d = 4\")\n",
    "matplot(Err, type=\"l\", xlab=\"Number of Trees\", lty=2, col=1:4,\n",
    "ylab=\"Classification Error Rate\")\n",
    "legend(\"topright\", legend=labels, col=1:4, lty=1)\n",
    "\n",
    "# View statistical reports \n",
    "colnames(Err) <- labels\n",
    "apply(Err, 2, summary)\n",
    "apply(Err[-c(1:100),], 2, summary)\n",
    "```\n",
    "\n",
    "<img src=\"Img/Boosting2.png\">\n",
    "<img src=\"Img/Boosting3.png\">\n",
    "\n",
    "- Because the Boosting model is a continuously updated model, the initially trained model is less accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded617d",
   "metadata": {},
   "source": [
    "# 6. Bayesian Additive Regression Trees \n",
    "\n",
    "- **Bayesian additive regression trees(BART)** is another ensemble method that uses decision trees as its building blocks. \n",
    "- **BART** methods combines other ensemble methods : \n",
    "    - Each tree is constructed in a random manner as in bagging and random forest. \n",
    "    - Each tree tries to capture signals not yet accounted for by the current model as in boosting. \n",
    "- **BART** method can be viewed as a Bayesian approach : \n",
    "    - $\\theta_1$ ~ $p(\\theta_1)$ : Prior distribution \n",
    "    - $\\theta_1 | \\theta_2, ... \\theta_k$ : Posterior distribution\n",
    "    - Each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from a posterior distribution. \n",
    "    - MCMC(Markov chain Monte Carlo) algorithm\n",
    "    - Remove out prediction results at burn-in period "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3ac9f",
   "metadata": {},
   "source": [
    "## 6.1 BART algorithms \n",
    "\n",
    "- $\\hat{f}^b_k(x)$ represents the prediction at $x$ for the $k$th tree used in the $b$th iteration : $k = 1, ..., K$ and $b = 1, ..., B$\n",
    "- Let $\\hat{f}^1_1(x) = ... = \\hat{f}^1_K(x) = \\frac{1}{nK}\\sum_{i=1}^n y_i$\n",
    "- Compute $\\hat{f}^1(x) = \\sum_{k=1}^K \\hat{f}^1_k(x)$ \n",
    "- For $b = 2, ..., B$ : \n",
    "    - For $k = 1, 2, ..., K$ : \n",
    "        - For $i = 1, ..., n$, compute the current partial residuals : $r_i = y_i - \\sum_{\\dot{k} < k} \\hat{f}$\n",
    "- Compute the mean after $L$ burn-in samples : $\\hat{f}(x) = \\frac{1}{B - L}\\sum_{b = L+1}^B \\hat{f}^b(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37c1d8",
   "metadata": {},
   "source": [
    "## 6.2 [Ex] BART : lbart/pbart\n",
    "\n",
    "```R\n",
    "# Prerequirisite \n",
    "library(BART)\n",
    "\n",
    "# Train-test split \n",
    "set.seed(123)\n",
    "train <- sample(1:nrow(Heart), nrow(Heart)/2)\n",
    "test <- setdiff(1:nrow(Heart), train)\n",
    "x <- Heart[, -14]\n",
    "y <- as.numeric(Heart[, 14])-1\n",
    "xtrain <- x[train, ]\n",
    "ytrain <- y[train]\n",
    "xtest <- x[-train, ]\n",
    "ytest <- y[-train]\n",
    "\n",
    "# Logistic BART \n",
    "set.seed(11)\n",
    "fit1 <- lbart(xtrain, ytrain, x.test=xtest)\n",
    "names(fit1)\n",
    "\n",
    "# Make predictions \n",
    "prob1 <- rep(0, length(ytest))\n",
    "prob1[fit1$prob.test.mean > 0.5] <- 1\n",
    "mean(prob1!=ytest)\n",
    "\n",
    "# Probit BART \n",
    "set.seed(22)\n",
    "fit2 <- pbart(xtrain, ytrain, x.test=xtest)\n",
    "\n",
    "# Make Prediction \n",
    "prob2 <- rep(0, length(ytest))\n",
    "prob2[fit2$prob.test.mean > 0.5] <- 1\n",
    "mean(prob2!=ytest)\n",
    "\n",
    "# Visualize results : lbart ~ pbart \n",
    "cbind(fit1$prob.test.mean, fit2$prob.test.mean)\n",
    "plot(fit1$prob.test.mean, fit2$prob.test.mean, col=ytest+2,\n",
    "xlab=\"Logistic BART\", ylab=\"Probit BART\")\n",
    "abline(0, 1, lty=3, col=\"grey\")\n",
    "abline(v=0.5, lty=1, col=\"grey\")\n",
    "abline(h=0.5, lty=1, col=\"grey\")\n",
    "legend(\"topleft\", col=c(2,3), pch=1,\n",
    "legend=c(\"AHD = No\", \"AHD = Yes\"))\n",
    "```\n",
    "\n",
    "<img src=\"Img/BART1.png\">\n",
    "\n",
    "- Misclassification error rate : 0.1946309"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61e1b3",
   "metadata": {},
   "source": [
    "## 6.4 [Ex] Comparision of MSE among different models : Tree, RF, Boosting, BART\n",
    "\n",
    "```R\n",
    "# Revisit Boston data set with a quantitative response\n",
    "library(MASS)\n",
    "summary(Boston)\n",
    "dim(Boston)\n",
    "\n",
    "# Train-test split\n",
    "set.seed(111)\n",
    "train <- sample(1:nrow(Boston), floor(nrow(Boston)*2/3))\n",
    "boston.test <- Boston[-train, \"medv\"]\n",
    "\n",
    "# Calculate misssclassification error rate of Regression tree\n",
    "library(tree)\n",
    "tree.boston <- tree(medv ~ ., Boston, subset=train)\n",
    "yhat <- predict(tree.boston, newdata=Boston[-train, ])\n",
    "mean((yhat - boston.test)^2)\n",
    "\n",
    "# Calculate missclassification error rate of LSE: least square estimates\n",
    "g0 <- lm(medv ~ ., Boston, subset=train)\n",
    "pred0 <- predict(g0, Boston[-train,])\n",
    "mean((pred0 - boston.test)^2)\n",
    "\n",
    "# Calculate missclassification error rate of Bagging\n",
    "library(randomForest)\n",
    "g1 <- randomForest(medv ~ ., data=Boston, mtry=13, subset=train)\n",
    "yhat1 <- predict(g1, newdata=Boston[-train, ])\n",
    "mean((yhat1 - boston.test)^2)\n",
    "\n",
    "# Calculate missclassification error rate of Random Forest (m = 4)\n",
    "g2 <- randomForest(medv ~ ., data=Boston, mtry=4, subset=train)\n",
    "yhat2 <- predict(g2, newdata=Boston[-train, ])\n",
    "mean((yhat2 - boston.test)^2)\n",
    "\n",
    "# Calculate missclassification error rate of Boosting (d = 4)\n",
    "library(gbm)\n",
    "g3 <- gbm(medv~., data = Boston[train, ], distribution=\"gaussian\", n.trees=5000, interaction.depth=4)\n",
    "yhat3 <- predict(g3, newdata=Boston[-train, ], n.trees=5000)\n",
    "mean((yhat3 - boston.test)^2)\n",
    "\n",
    "# Calculate missclassifcation error rate of BART\n",
    "library(BART)\n",
    "g4 <- gbart(Boston[train, 1:13], Boston[train, \"medv\"], x.test=Boston[-train, 1:13])\n",
    "yhat4 <- g4$yhat.test.mean\n",
    "mean((yhat4 - boston.test)^2)\n",
    "```\n",
    "\n",
    "|Regression tree|LSE|Bagging|Random Forest|Boosting|BART|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|24.95443|21.52607|12.43788|8.304427|12.2265|11.68592|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b83a7e",
   "metadata": {},
   "source": [
    "## 6.5 [Ex] Simulation study of 6.4 \n",
    "\n",
    "```R\n",
    "# Simulation: 4 ensemble methods\n",
    "set.seed(1111)\n",
    "N <- 20\n",
    "ERR <- matrix(0, N, 4)\n",
    "\n",
    "# replicate 20 times \n",
    "for (i in 1:N) {\n",
    "    train <- sample(1:nrow(Boston), floor(nrow(Boston)*2/3))\n",
    "    boston.test <- Boston[-train, \"medv\"]\n",
    "    \n",
    "    # Bagging\n",
    "    g1 <- randomForest(medv ~ ., data=Boston, mtry=13, subset=train)\n",
    "    yhat1 <- predict(g1, newdata=Boston[-train, ])\n",
    "    ERR[i,1] <- mean((yhat1 - boston.test)^2)\n",
    "\n",
    "    # Random forest\n",
    "    g2 <- randomForest(medv ~ ., data=Boston, mtry=4, subset=train)\n",
    "    yhat2 <- predict(g2, newdata=Boston[-train, ])\n",
    "    ERR[i, 2] <- mean((yhat2 - boston.test)^2)\n",
    "\n",
    "    # Boosting\n",
    "    g3 <- gbm(medv~., data = Boston[train, ], n.trees=5000, distribution=\"gaussian\", interaction.depth=4)\n",
    "    yhat3 <- predict(g3, newdata=Boston[-train, ], n.trees=5000)\n",
    "    ERR[i, 3] <- mean((yhat3 - boston.test)^2)\n",
    "\n",
    "    # BART\n",
    "    invisible(capture.output(g4 <- gbart(Boston[train, 1:13], Boston[train, \"medv\"], x.test=Boston[-train, 1:13])))\n",
    "    yhat4 <- g4$yhat.test.mean\n",
    "    ERR[i, 4] <- mean((yhat4 - boston.test)^2)\n",
    "}\n",
    "\n",
    "# Visualize simulation results \n",
    "labels <- c(\"Bagging\", \"RF\", \"Boosting\", \"BART\")\n",
    "boxplot(ERR, boxwex=0.5, main=\"Ensemble Methods\", col=2:5, names=labels, ylab=\"Mean Squared Errors\", ylim=c(0,30))\n",
    "colnames(ERR) <- labels\n",
    "\n",
    "# Check statistical reports \n",
    "apply(ERR, 2, summary)\n",
    "apply(ERR, 2, var)\n",
    "\n",
    "# Check rankings \n",
    "RA <- t(apply(ERR, 1, rank))\n",
    "RA\n",
    "apply(RA, 2, table)\n",
    "```\n",
    "\n",
    "<img src=\"Img/BART3.png\" width=\"400\">\n",
    "\n",
    "**<center>Mean of missclassification error rate** \n",
    "\n",
    "|Bagging|RF|Boosting|BART|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|12.265594|12.104095|11.323421|11.057668|\n",
    "\n",
    "**<center>Variance of missclassification error rate** \n",
    "\n",
    "|Bagging|RF|Boosting|BART|\n",
    "|:---:|:---:|:---:|:---:| \n",
    "|8.16965|12.691002|4.768590|7.754700|\n",
    "\n",
    "**<center>Ranking of simulations**\n",
    "\n",
    "|Ranks|Bagging|RF|Boosting|BART|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|1|3|3|8|6|\n",
    "|2|4|5|3|8|\n",
    "|3|6|3|6|5|\n",
    "|4|7|9|3|1|\n",
    "\n",
    "\n",
    "- The variance of boosting model is best. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "160px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
